DEBUG:flwr:Pre-registering run with id 1212822145635323044
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=5, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23033960180282592, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.18989687942266464, {'accuracy': 0.3634}, 29.551118600007612)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (2, 0.16582831588983535, {'accuracy': 0.4462}, 57.95604249997996)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 3]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (3, 0.15042347552776336, {'accuracy': 0.4684}, 88.05611479998333)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 4]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (4, 0.1399091305732727, {'accuracy': 0.504}, 117.43859549995977)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 5]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (5, 0.1306140089929104, {'accuracy': 0.5366}, 147.17213590000756)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 5 round(s) in 153.41s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.16296071043610574
INFO:flwr:		round 2: 0.1523495593369007
INFO:flwr:		round 3: 0.138825448282063
INFO:flwr:		round 4: 0.1399833646118641
INFO:flwr:		round 5: 0.1484494901895523
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.23033960180282592
INFO:flwr:		round 1: 0.18989687942266464
INFO:flwr:		round 2: 0.16582831588983535
INFO:flwr:		round 3: 0.15042347552776336
INFO:flwr:		round 4: 0.1399091305732727
INFO:flwr:		round 5: 0.1306140089929104
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.41325),
INFO:flwr:	         (2, 0.44825000000000004),
INFO:flwr:	         (3, 0.50675),
INFO:flwr:	         (4, 0.51025),
INFO:flwr:	         (5, 0.50075)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1),
INFO:flwr:	              (1, 0.3634),
INFO:flwr:	              (2, 0.4462),
INFO:flwr:	              (3, 0.4684),
INFO:flwr:	              (4, 0.504),
INFO:flwr:	              (5, 0.5366)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.

DEBUG:flwr:Pre-registering run with id 6249802506171341546
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=5, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23045606710910796, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.19330978780984878, {'accuracy': 0.406}, 32.076022499997634)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (2, 0.16504898389577866, {'accuracy': 0.4507}, 60.94195399997989)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 3]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (3, 0.14759613647460937, {'accuracy': 0.4896}, 87.51174560002983)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 4]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (4, 0.1338674701333046, {'accuracy': 0.5222}, 118.2188460000325)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 5]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (5, 0.12431417433917523, {'accuracy': 0.5583}, 145.46168690000195)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 5 round(s) in 151.20s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.15190616641938687
INFO:flwr:		round 2: 0.14329927144944665
INFO:flwr:		round 3: 0.13490942299365996
INFO:flwr:		round 4: 0.1407231750637293
INFO:flwr:		round 5: 0.13690045400336384
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.23045606710910796
INFO:flwr:		round 1: 0.19330978780984878
INFO:flwr:		round 2: 0.16504898389577866
INFO:flwr:		round 3: 0.14759613647460937
INFO:flwr:		round 4: 0.1338674701333046
INFO:flwr:		round 5: 0.12431417433917523
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.4495),
INFO:flwr:	         (2, 0.486),
INFO:flwr:	         (3, 0.5177499999999999),
INFO:flwr:	         (4, 0.507),
INFO:flwr:	         (5, 0.5277499999999999)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1),
INFO:flwr:	              (1, 0.406),
INFO:flwr:	              (2, 0.4507),
INFO:flwr:	              (3, 0.4896),
INFO:flwr:	              (4, 0.5222),
INFO:flwr:	              (5, 0.5583)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 14517267631525152809
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=5, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Pre-registering run with id 11633221916489345892
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=10, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.2305404418706894, {'accuracy': 0.0979}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.21028624707460403, {'accuracy': 0.2683}, 31.977241799992044)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (2, 0.19743965063095092, {'accuracy': 0.299}, 61.789090399979614)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 3]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (3, 0.17919247875213623, {'accuracy': 0.3627}, 91.51561399997445)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 4]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (4, 0.1494008836865425, {'accuracy': 0.4637}, 119.47511609998764)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 5]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (5, 0.139234975874424, {'accuracy': 0.5002}, 148.52636949997395)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 6]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (6, 0.13251162970066072, {'accuracy': 0.5254}, 175.44728129997384)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 7]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (7, 0.12643762893080712, {'accuracy': 0.55}, 208.81641759997)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 8]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (8, 0.11896117831766605, {'accuracy': 0.5783}, 236.06444360001478)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 9]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (9, 0.11582659567892552, {'accuracy': 0.5946}, 261.9590000999742)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 10]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (10, 0.1116987875059247, {'accuracy': 0.6062}, 290.06200440000976)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 10 round(s) in 297.42s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.15705586332082747
INFO:flwr:		round 2: 0.14532384923100472
INFO:flwr:		round 3: 0.14390566332638263
INFO:flwr:		round 4: 0.13820464849472047
INFO:flwr:		round 5: 0.14237276139855384
INFO:flwr:		round 6: 0.13388061670213935
INFO:flwr:		round 7: 0.13599468082934618
INFO:flwr:		round 8: 0.1296129371970892
INFO:flwr:		round 9: 0.12685613514482977
INFO:flwr:		round 10: 0.12473864360153675
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.2305404418706894
INFO:flwr:		round 1: 0.21028624707460403
INFO:flwr:		round 2: 0.19743965063095092
INFO:flwr:		round 3: 0.17919247875213623
INFO:flwr:		round 4: 0.1494008836865425
INFO:flwr:		round 5: 0.139234975874424
INFO:flwr:		round 6: 0.13251162970066072
INFO:flwr:		round 7: 0.12643762893080712
INFO:flwr:		round 8: 0.11896117831766605
INFO:flwr:		round 9: 0.11582659567892552
INFO:flwr:		round 10: 0.1116987875059247
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.43025),
INFO:flwr:	         (2, 0.4865),
INFO:flwr:	         (3, 0.48125),
INFO:flwr:	         (4, 0.51325),
INFO:flwr:	         (5, 0.5115),
INFO:flwr:	         (6, 0.53625),
INFO:flwr:	         (7, 0.5467500000000001),
INFO:flwr:	         (8, 0.5545),
INFO:flwr:	         (9, 0.5615),
INFO:flwr:	         (10, 0.56525)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.0979),
INFO:flwr:	              (1, 0.2683),
INFO:flwr:	              (2, 0.299),
INFO:flwr:	              (3, 0.3627),
INFO:flwr:	              (4, 0.4637),
INFO:flwr:	              (5, 0.5002),
INFO:flwr:	              (6, 0.5254),
INFO:flwr:	              (7, 0.55),
INFO:flwr:	              (8, 0.5783),
INFO:flwr:	              (9, 0.5946),
INFO:flwr:	              (10, 0.6062)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 7835608627256424354
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=5, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.2303936789751053, {'accuracy': 0.0996}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (1, 0.20090073617696763, {'accuracy': 0.3063}, 35.826404399995226)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (2, 0.17375853176116943, {'accuracy': 0.4043}, 63.51486350002233)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 3]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (3, 0.15508049449920655, {'accuracy': 0.4496}, 91.97331560001476)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 4]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (4, 0.14491378315091133, {'accuracy': 0.4758}, 125.27290949999588)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 5]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (5, 0.13907484589219093, {'accuracy': 0.4973}, 153.49569499999052)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 5 round(s) in 162.67s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.1686002173781395
INFO:flwr:		round 2: 0.15758044798374177
INFO:flwr:		round 3: 0.15460719777345655
INFO:flwr:		round 4: 0.1499232523083687
INFO:flwr:		round 5: 0.14666866933107378
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.2303936789751053
INFO:flwr:		round 1: 0.20090073617696763
INFO:flwr:		round 2: 0.17375853176116943
INFO:flwr:		round 3: 0.15508049449920655
INFO:flwr:		round 4: 0.14491378315091133
INFO:flwr:		round 5: 0.13907484589219093
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.3816),
INFO:flwr:	         (2, 0.4166),
INFO:flwr:	         (3, 0.4434),
INFO:flwr:	         (4, 0.45200000000000007),
INFO:flwr:	         (5, 0.48200000000000004)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.0996),
INFO:flwr:	              (1, 0.3063),
INFO:flwr:	              (2, 0.4043),
INFO:flwr:	              (3, 0.4496),
INFO:flwr:	              (4, 0.4758),
INFO:flwr:	              (5, 0.4973)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 7747922681964291114
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=5, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23039055979251863, {'accuracy': 0.1018}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (1, 0.21055678508281708, {'accuracy': 0.2821}, 30.251987399999052)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (2, 0.18747654218673707, {'accuracy': 0.3414}, 58.48449200001778)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 3]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (3, 0.16860301476716996, {'accuracy': 0.3999}, 87.51838309998857)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 4]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (4, 0.15161897915005684, {'accuracy': 0.4508}, 114.30095429997891)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 5]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (5, 0.1417224970340729, {'accuracy': 0.4961}, 143.9970345000038)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 5 round(s) in 151.85s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.18238037371635438
INFO:flwr:		round 2: 0.16177566108703614
INFO:flwr:		round 3: 0.15853914511203765
INFO:flwr:		round 4: 0.14873809822797773
INFO:flwr:		round 5: 0.1485700522184372
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.23039055979251863
INFO:flwr:		round 1: 0.21055678508281708
INFO:flwr:		round 2: 0.18747654218673707
INFO:flwr:		round 3: 0.16860301476716996
INFO:flwr:		round 4: 0.15161897915005684
INFO:flwr:		round 5: 0.1417224970340729
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.335),
INFO:flwr:	         (2, 0.41839999999999994),
INFO:flwr:	         (3, 0.42119999999999996),
INFO:flwr:	         (4, 0.4648),
INFO:flwr:	         (5, 0.4614)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1018),
INFO:flwr:	              (1, 0.2821),
INFO:flwr:	              (2, 0.3414),
INFO:flwr:	              (3, 0.3999),
INFO:flwr:	              (4, 0.4508),
INFO:flwr:	              (5, 0.4961)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 16403479860868208394
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=5, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23046083867549896, {'accuracy': 0.094}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
ERROR:flwr:ServerApp thread raised an exception: 'Parameters' object is not iterable
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso.py", line 453, in aggregate_fit
    total_bytes += sum(sys.getsizeof(param) for param in fit_res.parameters)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'Parameters' object is not iterable

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 1255696189761536389
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=5, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23029147095680236, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
ERROR:flwr:ServerApp thread raised an exception: 'int' object is not iterable
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso.py", line 453, in aggregate_fit
    total_bytes += sum(sys.getsizeof(fit_res.parameters))
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'int' object is not iterable

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 5796027549545123507
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=5, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.230599706697464, {'accuracy': 0.0979}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (1, 0.21107551749944686, {'accuracy': 0.2929}, 25.824561799992807)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 7af02660b902129e6469d3bd01000000
	pid: 4948
	namespace: 99045bbc-3603-45ae-a957-da50501ad5af
	ip: 127.0.0.1
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 10054. An existing connection was forcibly closed by the remote host. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
DEBUG:flwr:Pre-registering run with id 10976787230281270399
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.2304006959438324, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (1, 0.19194234030246735, {'accuracy': 0.355}, 29.215488900023047)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 1 round(s) in 38.78s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.16693905377388
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.2304006959438324
INFO:flwr:		round 1: 0.19194234030246735
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.39160000000000006)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1), (1, 0.355)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 954204714387600321
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23049604473114013, {'accuracy': 0.1016}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27756, ip=127.0.0.1, actor_id=d7941946d11d45e5830902a901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D415EF1A10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27756, ip=127.0.0.1, actor_id=d7941946d11d45e5830902a901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D415EF1A10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27756, ip=127.0.0.1, actor_id=d7941946d11d45e5830902a901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D415EF1A10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27756, ip=127.0.0.1, actor_id=d7941946d11d45e5830902a901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D415EF1A10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=24180, ip=127.0.0.1, actor_id=36ed0c1d35bc2918700df83901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002782E2FC790>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24180, ip=127.0.0.1, actor_id=36ed0c1d35bc2918700df83901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002782E2FC790>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=24180, ip=127.0.0.1, actor_id=36ed0c1d35bc2918700df83901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002782E2FC790>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24180, ip=127.0.0.1, actor_id=36ed0c1d35bc2918700df83901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002782E2FC790>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27740, ip=127.0.0.1, actor_id=732f6f93c7f295ec4dd0ab9101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000028389EFD890>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27740, ip=127.0.0.1, actor_id=732f6f93c7f295ec4dd0ab9101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000028389EFD890>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27740, ip=127.0.0.1, actor_id=732f6f93c7f295ec4dd0ab9101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000028389EFD890>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27740, ip=127.0.0.1, actor_id=732f6f93c7f295ec4dd0ab9101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000028389EFD890>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=26196, ip=127.0.0.1, actor_id=8da88d2cdc2340aff40750c001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A272BCC790>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=26196, ip=127.0.0.1, actor_id=8da88d2cdc2340aff40750c001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A272BCC790>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=26196, ip=127.0.0.1, actor_id=8da88d2cdc2340aff40750c001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A272BCC790>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=26196, ip=127.0.0.1, actor_id=8da88d2cdc2340aff40750c001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A272BCC790>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27656, ip=127.0.0.1, actor_id=6ad0bf97f8fea07a10f37cc301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A87823C510>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27656, ip=127.0.0.1, actor_id=6ad0bf97f8fea07a10f37cc301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A87823C510>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27656, ip=127.0.0.1, actor_id=6ad0bf97f8fea07a10f37cc301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A87823C510>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27656, ip=127.0.0.1, actor_id=6ad0bf97f8fea07a10f37cc301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A87823C510>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27788, ip=127.0.0.1, actor_id=eab4bed095b89efdbc38eacd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014178E81FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27788, ip=127.0.0.1, actor_id=eab4bed095b89efdbc38eacd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014178E81FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27788, ip=127.0.0.1, actor_id=eab4bed095b89efdbc38eacd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014178E81FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27788, ip=127.0.0.1, actor_id=eab4bed095b89efdbc38eacd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014178E81FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27560, ip=127.0.0.1, actor_id=f2fa4e6cc61402775bbf223101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000179A078C790>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27560, ip=127.0.0.1, actor_id=f2fa4e6cc61402775bbf223101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000179A078C790>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27560, ip=127.0.0.1, actor_id=f2fa4e6cc61402775bbf223101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000179A078C790>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27560, ip=127.0.0.1, actor_id=f2fa4e6cc61402775bbf223101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000179A078C790>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27584, ip=127.0.0.1, actor_id=d0410065579cc34e6b542f1901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F738CD9FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27584, ip=127.0.0.1, actor_id=d0410065579cc34e6b542f1901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F738CD9FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27584, ip=127.0.0.1, actor_id=d0410065579cc34e6b542f1901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F738CD9FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27584, ip=127.0.0.1, actor_id=d0410065579cc34e6b542f1901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F738CD9FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=24924, ip=127.0.0.1, actor_id=7f952f93d9c4de5b521c9aba01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FA59B0C790>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24924, ip=127.0.0.1, actor_id=7f952f93d9c4de5b521c9aba01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FA59B0C790>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=24924, ip=127.0.0.1, actor_id=7f952f93d9c4de5b521c9aba01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FA59B0C790>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24924, ip=127.0.0.1, actor_id=7f952f93d9c4de5b521c9aba01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FA59B0C790>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27776, ip=127.0.0.1, actor_id=0c0ea86c577668529cd3bd4401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F436E9C790>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27776, ip=127.0.0.1, actor_id=0c0ea86c577668529cd3bd4401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F436E9C790>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27776, ip=127.0.0.1, actor_id=0c0ea86c577668529cd3bd4401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F436E9C790>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 312, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 243, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27776, ip=127.0.0.1, actor_id=0c0ea86c577668529cd3bd4401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F436E9C790>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

INFO:flwr:aggregate_fit: received 0 results and 10 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 470, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 195, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 13075084133288799939
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.2304264005422592, {'accuracy': 0.0918}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=29420, ip=127.0.0.1, actor_id=153ac62a222cca5de19f504901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000205ECFB7F10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29420, ip=127.0.0.1, actor_id=153ac62a222cca5de19f504901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000205ECFB7F10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=22440, ip=127.0.0.1, actor_id=2b28b984383c207d4d15e82c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001992A084E90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22440, ip=127.0.0.1, actor_id=2b28b984383c207d4d15e82c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001992A084E90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=29420, ip=127.0.0.1, actor_id=153ac62a222cca5de19f504901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000205ECFB7F10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29420, ip=127.0.0.1, actor_id=153ac62a222cca5de19f504901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000205ECFB7F10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=22440, ip=127.0.0.1, actor_id=2b28b984383c207d4d15e82c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001992A084E90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22440, ip=127.0.0.1, actor_id=2b28b984383c207d4d15e82c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001992A084E90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=11168, ip=127.0.0.1, actor_id=19172c415514243c9cf6fe1701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F409D62690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=11168, ip=127.0.0.1, actor_id=19172c415514243c9cf6fe1701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F409D62690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=11168, ip=127.0.0.1, actor_id=19172c415514243c9cf6fe1701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F409D62690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=11168, ip=127.0.0.1, actor_id=19172c415514243c9cf6fe1701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F409D62690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=22940, ip=127.0.0.1, actor_id=71a4359633d09d66b936a85301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B589177D10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22940, ip=127.0.0.1, actor_id=71a4359633d09d66b936a85301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B589177D10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=22940, ip=127.0.0.1, actor_id=71a4359633d09d66b936a85301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B589177D10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22940, ip=127.0.0.1, actor_id=71a4359633d09d66b936a85301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B589177D10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=15372, ip=127.0.0.1, actor_id=2b634fec0e8d459dc7e9db4c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000283BC7620D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=15372, ip=127.0.0.1, actor_id=2b634fec0e8d459dc7e9db4c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000283BC7620D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=15372, ip=127.0.0.1, actor_id=2b634fec0e8d459dc7e9db4c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000283BC7620D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=15372, ip=127.0.0.1, actor_id=2b634fec0e8d459dc7e9db4c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000283BC7620D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=13540, ip=127.0.0.1, actor_id=ad4db59ecdea1a74d7d9215801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000200F4312290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13540, ip=127.0.0.1, actor_id=ad4db59ecdea1a74d7d9215801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000200F4312290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=13540, ip=127.0.0.1, actor_id=ad4db59ecdea1a74d7d9215801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000200F4312290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13540, ip=127.0.0.1, actor_id=ad4db59ecdea1a74d7d9215801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000200F4312290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=25636, ip=127.0.0.1, actor_id=379ae67921a1befc678dc63d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002768CAFBC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=25636, ip=127.0.0.1, actor_id=379ae67921a1befc678dc63d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002768CAFBC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=25636, ip=127.0.0.1, actor_id=379ae67921a1befc678dc63d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002768CAFBC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=25636, ip=127.0.0.1, actor_id=379ae67921a1befc678dc63d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002768CAFBC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=28196, ip=127.0.0.1, actor_id=033a1885feaef8b50841f19501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001477ECC1FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=28196, ip=127.0.0.1, actor_id=033a1885feaef8b50841f19501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001477ECC1FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=28196, ip=127.0.0.1, actor_id=033a1885feaef8b50841f19501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001477ECC1FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=28196, ip=127.0.0.1, actor_id=033a1885feaef8b50841f19501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001477ECC1FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=20784, ip=127.0.0.1, actor_id=2b605e3f1b6d928279cb50d501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E43E04A290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20784, ip=127.0.0.1, actor_id=2b605e3f1b6d928279cb50d501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E43E04A290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=20784, ip=127.0.0.1, actor_id=2b605e3f1b6d928279cb50d501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E43E04A290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20784, ip=127.0.0.1, actor_id=2b605e3f1b6d928279cb50d501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E43E04A290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=22936, ip=127.0.0.1, actor_id=3c510f9743d34bdf9265a4e501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022F24D92690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22936, ip=127.0.0.1, actor_id=3c510f9743d34bdf9265a4e501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022F24D92690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=22936, ip=127.0.0.1, actor_id=3c510f9743d34bdf9265a4e501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022F24D92690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 317, in fit
    pruned_params = prune_model(net_quantized, 0.5)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 248, in prune_model
    parameters = get_parameters(net)
                 ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in get_parameters
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 189, in <listcomp>
    return [val.cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^^^^
TypeError: Got unsupported ScalarType QInt8

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22936, ip=127.0.0.1, actor_id=3c510f9743d34bdf9265a4e501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022F24D92690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Got unsupported ScalarType QInt8

INFO:flwr:aggregate_fit: received 0 results and 10 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 475, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 200, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 9989222776111676581
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.2306015541791916, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=12540, ip=127.0.0.1, actor_id=6de5b862ec23fd0c47f5c34b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000186BEC81FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=12540, ip=127.0.0.1, actor_id=6de5b862ec23fd0c47f5c34b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000186BEC81FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=12540, ip=127.0.0.1, actor_id=6de5b862ec23fd0c47f5c34b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000186BEC81FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=12540, ip=127.0.0.1, actor_id=6de5b862ec23fd0c47f5c34b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000186BEC81FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21416, ip=127.0.0.1, actor_id=29d94eea814724aa22f6f75301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B57DFB8ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21416, ip=127.0.0.1, actor_id=29d94eea814724aa22f6f75301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B57DFB8ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21416, ip=127.0.0.1, actor_id=29d94eea814724aa22f6f75301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B57DFB8ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21416, ip=127.0.0.1, actor_id=29d94eea814724aa22f6f75301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B57DFB8ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=23672, ip=127.0.0.1, actor_id=a258e81a3d3a3d2adae0315201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A6638E1190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23672, ip=127.0.0.1, actor_id=a258e81a3d3a3d2adae0315201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A6638E1190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=23672, ip=127.0.0.1, actor_id=a258e81a3d3a3d2adae0315201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A6638E1190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23672, ip=127.0.0.1, actor_id=a258e81a3d3a3d2adae0315201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A6638E1190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=23132, ip=127.0.0.1, actor_id=dccca680df440561f3b40ea801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001BB56EAAF90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23132, ip=127.0.0.1, actor_id=dccca680df440561f3b40ea801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001BB56EAAF90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=23132, ip=127.0.0.1, actor_id=dccca680df440561f3b40ea801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001BB56EAAF90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23132, ip=127.0.0.1, actor_id=dccca680df440561f3b40ea801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001BB56EAAF90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=20112, ip=127.0.0.1, actor_id=0e44b6359f0df9ddcd1db89101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000280794DD5D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20112, ip=127.0.0.1, actor_id=0e44b6359f0df9ddcd1db89101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000280794DD5D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=20112, ip=127.0.0.1, actor_id=0e44b6359f0df9ddcd1db89101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000280794DD5D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20112, ip=127.0.0.1, actor_id=0e44b6359f0df9ddcd1db89101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000280794DD5D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=19508, ip=127.0.0.1, actor_id=6c071abe40c5a84166aeb88301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000294916D1A10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=19508, ip=127.0.0.1, actor_id=6c071abe40c5a84166aeb88301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000294916D1A10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=19508, ip=127.0.0.1, actor_id=6c071abe40c5a84166aeb88301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000294916D1A10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=19508, ip=127.0.0.1, actor_id=6c071abe40c5a84166aeb88301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000294916D1A10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=30484, ip=127.0.0.1, actor_id=3661217a1cd9fbd1aeae057c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ABC1D7BF10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30484, ip=127.0.0.1, actor_id=3661217a1cd9fbd1aeae057c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ABC1D7BF10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=30484, ip=127.0.0.1, actor_id=3661217a1cd9fbd1aeae057c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ABC1D7BF10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30484, ip=127.0.0.1, actor_id=3661217a1cd9fbd1aeae057c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ABC1D7BF10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=3268, ip=127.0.0.1, actor_id=1c273f634c376f159243118301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017AC77E1FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=3268, ip=127.0.0.1, actor_id=1c273f634c376f159243118301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017AC77E1FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=3268, ip=127.0.0.1, actor_id=1c273f634c376f159243118301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017AC77E1FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=3268, ip=127.0.0.1, actor_id=1c273f634c376f159243118301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017AC77E1FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=23092, ip=127.0.0.1, actor_id=0f3b179d38efc0a5ab7cfe8901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001CE649B4AD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23092, ip=127.0.0.1, actor_id=0f3b179d38efc0a5ab7cfe8901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001CE649B4AD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=23092, ip=127.0.0.1, actor_id=0f3b179d38efc0a5ab7cfe8901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001CE649B4AD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23092, ip=127.0.0.1, actor_id=0f3b179d38efc0a5ab7cfe8901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001CE649B4AD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=18928, ip=127.0.0.1, actor_id=e8b762b71af3bdceb9189ba801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FF8DEDBC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18928, ip=127.0.0.1, actor_id=e8b762b71af3bdceb9189ba801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FF8DEDBC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=18928, ip=127.0.0.1, actor_id=e8b762b71af3bdceb9189ba801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FF8DEDBC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 318, in fit
    ndarrays_updated = get_quantised_parameters(self.client_model)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in get_quantised_parameters
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 194, in <listcomp>
    return [val.int_repr().cpu().numpy() for _, val in net.state_dict().items()]
            ^^^^^^^^^^^^^^
NotImplementedError: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]


The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18928, ip=127.0.0.1, actor_id=e8b762b71af3bdceb9189ba801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FF8DEDBC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Could not run 'aten::int_repr' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::int_repr' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCPU.cpp:955 [kernel]
QuantizedCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen\RegisterQuantizedCUDA.cpp:463 [kernel]
BackendSelect: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:194 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:503 [backend fallback]
Functionalize: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\VariableFallbackKernel.cpp:100 [backend fallback]
AutogradOther: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradCUDA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHIP: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXLA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMPS: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradIPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradXPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradHPU: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradVE: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradLazy: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMTIA: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse1: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse2: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradPrivateUse3: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradMeta: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
AutogradNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\VariableType_0.cpp:18082 [autograd kernel]
Tracer: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\torch\csrc\autograd\generated\TraceType_0.cpp:17100 [kernel]
AutocastCPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:322 [backend fallback]
AutocastXPU: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:465 [backend fallback]
AutocastMPS: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:202 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\functorch\DynamicLayer.cpp:499 [backend fallback]
PreDispatch: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:206 [backend fallback]
PythonDispatcher: registered at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\core\PythonFallbackKernel.cpp:198 [backend fallback]

INFO:flwr:aggregate_fit: received 0 results and 10 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 470, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 200, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 7154300505349309274
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
INFO:flwr:initial parameters (loss, other metrics): 0.23036968002319336, {'accuracy': 0.1013}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=30092, ip=127.0.0.1, actor_id=fc24fa8bc04214a4cf71014401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002467A6A2690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30092, ip=127.0.0.1, actor_id=fc24fa8bc04214a4cf71014401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002467A6A2690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=29744, ip=127.0.0.1, actor_id=284e2e8ac5bc898350f57fed01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002846DF6BC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29744, ip=127.0.0.1, actor_id=284e2e8ac5bc898350f57fed01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002846DF6BC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=30092, ip=127.0.0.1, actor_id=fc24fa8bc04214a4cf71014401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002467A6A2690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30092, ip=127.0.0.1, actor_id=fc24fa8bc04214a4cf71014401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002467A6A2690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=29744, ip=127.0.0.1, actor_id=284e2e8ac5bc898350f57fed01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002846DF6BC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29744, ip=127.0.0.1, actor_id=284e2e8ac5bc898350f57fed01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002846DF6BC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=8632, ip=127.0.0.1, actor_id=1cb49f3dba09f12423e2a7d801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022FA4D97D10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=8632, ip=127.0.0.1, actor_id=1cb49f3dba09f12423e2a7d801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022FA4D97D10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=8632, ip=127.0.0.1, actor_id=1cb49f3dba09f12423e2a7d801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022FA4D97D10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=8632, ip=127.0.0.1, actor_id=1cb49f3dba09f12423e2a7d801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022FA4D97D10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=5060, ip=127.0.0.1, actor_id=7b908e67900da4f5bcba3e6001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001386CB89AD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=5060, ip=127.0.0.1, actor_id=7b908e67900da4f5bcba3e6001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001386CB89AD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=5060, ip=127.0.0.1, actor_id=7b908e67900da4f5bcba3e6001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001386CB89AD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=5060, ip=127.0.0.1, actor_id=7b908e67900da4f5bcba3e6001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001386CB89AD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=12768, ip=127.0.0.1, actor_id=28edd48dee4c3b795a07cd4f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001BA02DFA210>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=12768, ip=127.0.0.1, actor_id=28edd48dee4c3b795a07cd4f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001BA02DFA210>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=12768, ip=127.0.0.1, actor_id=28edd48dee4c3b795a07cd4f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001BA02DFA210>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=12768, ip=127.0.0.1, actor_id=28edd48dee4c3b795a07cd4f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001BA02DFA210>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=25548, ip=127.0.0.1, actor_id=96efd728966ec954dc21a41101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027173AE4290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=25548, ip=127.0.0.1, actor_id=96efd728966ec954dc21a41101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027173AE4290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=25548, ip=127.0.0.1, actor_id=96efd728966ec954dc21a41101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027173AE4290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=25548, ip=127.0.0.1, actor_id=96efd728966ec954dc21a41101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027173AE4290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27956, ip=127.0.0.1, actor_id=17a958d1005ca22b450f511801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002394FC99ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27956, ip=127.0.0.1, actor_id=17a958d1005ca22b450f511801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002394FC99ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27956, ip=127.0.0.1, actor_id=17a958d1005ca22b450f511801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002394FC99ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27956, ip=127.0.0.1, actor_id=17a958d1005ca22b450f511801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002394FC99ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=24976, ip=127.0.0.1, actor_id=b9bfb605ffc3f506f1390f5901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021DB843A690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24976, ip=127.0.0.1, actor_id=b9bfb605ffc3f506f1390f5901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021DB843A690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=24976, ip=127.0.0.1, actor_id=b9bfb605ffc3f506f1390f5901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021DB843A690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24976, ip=127.0.0.1, actor_id=b9bfb605ffc3f506f1390f5901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021DB843A690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=18596, ip=127.0.0.1, actor_id=09e7eca7ee3f358942275f8501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000015CB354BC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18596, ip=127.0.0.1, actor_id=09e7eca7ee3f358942275f8501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000015CB354BC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=18596, ip=127.0.0.1, actor_id=09e7eca7ee3f358942275f8501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000015CB354BC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18596, ip=127.0.0.1, actor_id=09e7eca7ee3f358942275f8501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000015CB354BC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=2368, ip=127.0.0.1, actor_id=3ff44877d022c1d054305fc801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000219F3041890>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2368, ip=127.0.0.1, actor_id=3ff44877d022c1d054305fc801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000219F3041890>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2368, ip=127.0.0.1, actor_id=3ff44877d022c1d054305fc801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000219F3041890>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 300, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_updated)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 66, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 89, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2368, ip=127.0.0.1, actor_id=3ff44877d022c1d054305fc801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000219F3041890>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

INFO:flwr:aggregate_fit: received 0 results and 10 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 451, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 191, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 13605022547395894860
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.2302974981546402, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=2392, ip=127.0.0.1, actor_id=2ce64f8ddabbb9af16f3c2e501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E194C58ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2392, ip=127.0.0.1, actor_id=2ce64f8ddabbb9af16f3c2e501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E194C58ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=9280, ip=127.0.0.1, actor_id=828dd4852b4a29292c33d9db01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018438D54810>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9280, ip=127.0.0.1, actor_id=828dd4852b4a29292c33d9db01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018438D54810>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=9280, ip=127.0.0.1, actor_id=828dd4852b4a29292c33d9db01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018438D54810>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9280, ip=127.0.0.1, actor_id=828dd4852b4a29292c33d9db01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018438D54810>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=2392, ip=127.0.0.1, actor_id=2ce64f8ddabbb9af16f3c2e501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E194C58ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=2392, ip=127.0.0.1, actor_id=2ce64f8ddabbb9af16f3c2e501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E194C58ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=26240, ip=127.0.0.1, actor_id=222adca55f21c3a4bb1076ee01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000200A5362690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=26240, ip=127.0.0.1, actor_id=222adca55f21c3a4bb1076ee01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000200A5362690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=26240, ip=127.0.0.1, actor_id=222adca55f21c3a4bb1076ee01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000200A5362690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=26240, ip=127.0.0.1, actor_id=222adca55f21c3a4bb1076ee01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000200A5362690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=18540, ip=127.0.0.1, actor_id=84b4ba7f02bc294a6018762201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D667F98ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18540, ip=127.0.0.1, actor_id=84b4ba7f02bc294a6018762201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D667F98ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=18540, ip=127.0.0.1, actor_id=84b4ba7f02bc294a6018762201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D667F98ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18540, ip=127.0.0.1, actor_id=84b4ba7f02bc294a6018762201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D667F98ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=8452, ip=127.0.0.1, actor_id=e97a6052c88abb91294f536e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FA97823C50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=8452, ip=127.0.0.1, actor_id=e97a6052c88abb91294f536e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FA97823C50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=8452, ip=127.0.0.1, actor_id=e97a6052c88abb91294f536e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FA97823C50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=8452, ip=127.0.0.1, actor_id=e97a6052c88abb91294f536e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FA97823C50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21136, ip=127.0.0.1, actor_id=7a14a1e70f3a9b83dc7bc2ae01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000136DF15A390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21136, ip=127.0.0.1, actor_id=7a14a1e70f3a9b83dc7bc2ae01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000136DF15A390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21136, ip=127.0.0.1, actor_id=7a14a1e70f3a9b83dc7bc2ae01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000136DF15A390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21136, ip=127.0.0.1, actor_id=7a14a1e70f3a9b83dc7bc2ae01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000136DF15A390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=16036, ip=127.0.0.1, actor_id=eedd923d60ecbe9d61a5393d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022BC9E64890>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16036, ip=127.0.0.1, actor_id=eedd923d60ecbe9d61a5393d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022BC9E64890>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=16036, ip=127.0.0.1, actor_id=eedd923d60ecbe9d61a5393d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022BC9E64890>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16036, ip=127.0.0.1, actor_id=eedd923d60ecbe9d61a5393d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022BC9E64890>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21572, ip=127.0.0.1, actor_id=4feab1ea27227b87705a9e6d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002C1AABB1C90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21572, ip=127.0.0.1, actor_id=4feab1ea27227b87705a9e6d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002C1AABB1C90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21572, ip=127.0.0.1, actor_id=4feab1ea27227b87705a9e6d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002C1AABB1C90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21572, ip=127.0.0.1, actor_id=4feab1ea27227b87705a9e6d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002C1AABB1C90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=24376, ip=127.0.0.1, actor_id=e743380fadf4a8e7465bc4c701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000275F9C9F3D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24376, ip=127.0.0.1, actor_id=e743380fadf4a8e7465bc4c701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000275F9C9F3D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=24376, ip=127.0.0.1, actor_id=e743380fadf4a8e7465bc4c701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000275F9C9F3D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24376, ip=127.0.0.1, actor_id=e743380fadf4a8e7465bc4c701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000275F9C9F3D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=8748, ip=127.0.0.1, actor_id=22375fc4cfbae5b47546d54801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E60787D5D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=8748, ip=127.0.0.1, actor_id=22375fc4cfbae5b47546d54801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E60787D5D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=8748, ip=127.0.0.1, actor_id=22375fc4cfbae5b47546d54801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E60787D5D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 292, in fit
    loss, acc = test(self.client_model, self.valloader)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 232, in test
    outputs = net(images)
              ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 180, in forward
    x = x.view(-1, 16 * 5 * 5)
        ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=8748, ip=127.0.0.1, actor_id=22375fc4cfbae5b47546d54801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E60787D5D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

INFO:flwr:aggregate_fit: received 0 results and 10 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 453, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 195, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 5469738688058162442
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23056358830928803, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=28084, ip=127.0.0.1, actor_id=cee11ec908190e7194d9124701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002541CEC9ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=28084, ip=127.0.0.1, actor_id=cee11ec908190e7194d9124701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002541CEC9ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=28084, ip=127.0.0.1, actor_id=cee11ec908190e7194d9124701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002541CEC9ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=28084, ip=127.0.0.1, actor_id=cee11ec908190e7194d9124701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002541CEC9ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=9184, ip=127.0.0.1, actor_id=8b33988fe0177dd32a1dfaf301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FD78F536D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9184, ip=127.0.0.1, actor_id=8b33988fe0177dd32a1dfaf301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FD78F536D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=9184, ip=127.0.0.1, actor_id=8b33988fe0177dd32a1dfaf301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FD78F536D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9184, ip=127.0.0.1, actor_id=8b33988fe0177dd32a1dfaf301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FD78F536D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=11448, ip=127.0.0.1, actor_id=93d033e116f0b845ab2e180101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027F756FE090>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=11448, ip=127.0.0.1, actor_id=93d033e116f0b845ab2e180101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027F756FE090>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=11448, ip=127.0.0.1, actor_id=93d033e116f0b845ab2e180101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027F756FE090>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=11448, ip=127.0.0.1, actor_id=93d033e116f0b845ab2e180101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027F756FE090>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=8448, ip=127.0.0.1, actor_id=c287aea85cb4e379a55ff72101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001EE1575B9D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=8448, ip=127.0.0.1, actor_id=c287aea85cb4e379a55ff72101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001EE1575B9D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=8448, ip=127.0.0.1, actor_id=c287aea85cb4e379a55ff72101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001EE1575B9D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=8448, ip=127.0.0.1, actor_id=c287aea85cb4e379a55ff72101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001EE1575B9D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=23060, ip=127.0.0.1, actor_id=fb85ec40abc1e25a43ac27f701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000225F2657D10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23060, ip=127.0.0.1, actor_id=fb85ec40abc1e25a43ac27f701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000225F2657D10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=23060, ip=127.0.0.1, actor_id=fb85ec40abc1e25a43ac27f701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000225F2657D10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23060, ip=127.0.0.1, actor_id=fb85ec40abc1e25a43ac27f701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000225F2657D10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=7332, ip=127.0.0.1, actor_id=9221f1bc02cbddd76663392701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000020113D9F3D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=7332, ip=127.0.0.1, actor_id=9221f1bc02cbddd76663392701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000020113D9F3D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=7332, ip=127.0.0.1, actor_id=9221f1bc02cbddd76663392701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000020113D9F3D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=7332, ip=127.0.0.1, actor_id=9221f1bc02cbddd76663392701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000020113D9F3D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=13116, ip=127.0.0.1, actor_id=e7970224bae74c830d18488301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000268AA679ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13116, ip=127.0.0.1, actor_id=e7970224bae74c830d18488301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000268AA679ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=13116, ip=127.0.0.1, actor_id=e7970224bae74c830d18488301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000268AA679ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13116, ip=127.0.0.1, actor_id=e7970224bae74c830d18488301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000268AA679ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=14592, ip=127.0.0.1, actor_id=6b128b49cadbff31b1f9cf0401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001874A21B4D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=14592, ip=127.0.0.1, actor_id=6b128b49cadbff31b1f9cf0401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001874A21B4D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=14592, ip=127.0.0.1, actor_id=6b128b49cadbff31b1f9cf0401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001874A21B4D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=14592, ip=127.0.0.1, actor_id=6b128b49cadbff31b1f9cf0401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001874A21B4D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21660, ip=127.0.0.1, actor_id=c5f0efd9d39fa737cef8b23a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014532278ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21660, ip=127.0.0.1, actor_id=c5f0efd9d39fa737cef8b23a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014532278ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21660, ip=127.0.0.1, actor_id=c5f0efd9d39fa737cef8b23a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014532278ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21660, ip=127.0.0.1, actor_id=c5f0efd9d39fa737cef8b23a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014532278ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=20564, ip=127.0.0.1, actor_id=fb25e704616fa1d62d38735001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000199C55B9FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20564, ip=127.0.0.1, actor_id=fb25e704616fa1d62d38735001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000199C55B9FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=20564, ip=127.0.0.1, actor_id=fb25e704616fa1d62d38735001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000199C55B9FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 297, in fit
    if param.dtype == torch.qint8:
       ^^^^^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'dtype'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20564, ip=127.0.0.1, actor_id=fb25e704616fa1d62d38735001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000199C55B9FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'torch.dtype' object has no attribute 'dtype'

INFO:flwr:aggregate_fit: received 0 results and 10 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 453, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 195, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 3573087617272916486
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
ERROR:flwr:ServerApp thread raised an exception: 'torch.dtype' object has no attribute 'numpy'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 93, in fit
    self.parameters = self._get_initial_parameters(server_round=0, timeout=timeout)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 273, in _get_initial_parameters
    parameters: Optional[Parameters] = self.strategy.initialize_parameters(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 311, in initialize_parameters
    weights = get_parameters(self.global_model)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 125, in get_parameters
    return [p.numpy() for p in quantized_model.state_dict().values()]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 125, in <listcomp>
    return [p.numpy() for p in quantized_model.state_dict().values()]
            ^^^^^^^
AttributeError: 'torch.dtype' object has no attribute 'numpy'

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 3455560538424195194
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23050056364536287, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=14356, ip=127.0.0.1, actor_id=cf37d705735c0c8db4d1dc6e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019946C373D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=14356, ip=127.0.0.1, actor_id=cf37d705735c0c8db4d1dc6e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019946C373D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=14356, ip=127.0.0.1, actor_id=cf37d705735c0c8db4d1dc6e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019946C373D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=14356, ip=127.0.0.1, actor_id=cf37d705735c0c8db4d1dc6e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019946C373D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=16208, ip=127.0.0.1, actor_id=9a6083b98e6363d52fede36601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C0CDA2690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16208, ip=127.0.0.1, actor_id=9a6083b98e6363d52fede36601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C0CDA2690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=16208, ip=127.0.0.1, actor_id=9a6083b98e6363d52fede36601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C0CDA2690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16208, ip=127.0.0.1, actor_id=9a6083b98e6363d52fede36601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C0CDA2690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=51d186c6be1868fea16572d201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014276678ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=51d186c6be1868fea16572d201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014276678ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=51d186c6be1868fea16572d201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014276678ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=51d186c6be1868fea16572d201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000014276678ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=28344, ip=127.0.0.1, actor_id=dbef2a0de80f3925234519cc01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000257088D2690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=28344, ip=127.0.0.1, actor_id=dbef2a0de80f3925234519cc01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000257088D2690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=28344, ip=127.0.0.1, actor_id=dbef2a0de80f3925234519cc01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000257088D2690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=28344, ip=127.0.0.1, actor_id=dbef2a0de80f3925234519cc01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000257088D2690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27684, ip=127.0.0.1, actor_id=c223ce68c32d7265880ece6801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002E405C592D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27684, ip=127.0.0.1, actor_id=c223ce68c32d7265880ece6801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002E405C592D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27684, ip=127.0.0.1, actor_id=c223ce68c32d7265880ece6801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002E405C592D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27684, ip=127.0.0.1, actor_id=c223ce68c32d7265880ece6801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002E405C592D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21008, ip=127.0.0.1, actor_id=d998880cf0c54d26a0b847fa01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002D2BE31AF90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21008, ip=127.0.0.1, actor_id=d998880cf0c54d26a0b847fa01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002D2BE31AF90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21008, ip=127.0.0.1, actor_id=d998880cf0c54d26a0b847fa01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002D2BE31AF90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21008, ip=127.0.0.1, actor_id=d998880cf0c54d26a0b847fa01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002D2BE31AF90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=552, ip=127.0.0.1, actor_id=c2f21ca1cec66d4c2ca35e9001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017B7B55F3D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=552, ip=127.0.0.1, actor_id=c2f21ca1cec66d4c2ca35e9001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017B7B55F3D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=552, ip=127.0.0.1, actor_id=c2f21ca1cec66d4c2ca35e9001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017B7B55F3D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=552, ip=127.0.0.1, actor_id=c2f21ca1cec66d4c2ca35e9001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017B7B55F3D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=11168, ip=127.0.0.1, actor_id=2f1bb6d3702b242c699172b901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017C471B9FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=11168, ip=127.0.0.1, actor_id=2f1bb6d3702b242c699172b901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017C471B9FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=11168, ip=127.0.0.1, actor_id=2f1bb6d3702b242c699172b901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017C471B9FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=11168, ip=127.0.0.1, actor_id=2f1bb6d3702b242c699172b901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000017C471B9FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=29268, ip=127.0.0.1, actor_id=a4b7dfa97a4c90c743a2d72301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A38AE21C90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29268, ip=127.0.0.1, actor_id=a4b7dfa97a4c90c743a2d72301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A38AE21C90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=29268, ip=127.0.0.1, actor_id=a4b7dfa97a4c90c743a2d72301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A38AE21C90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29268, ip=127.0.0.1, actor_id=a4b7dfa97a4c90c743a2d72301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A38AE21C90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=23612, ip=127.0.0.1, actor_id=cdfe7c7c7c74613bc2da878901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000218A42B26D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23612, ip=127.0.0.1, actor_id=cdfe7c7c7c74613bc2da878901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000218A42B26D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=23612, ip=127.0.0.1, actor_id=cdfe7c7c7c74613bc2da878901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000218A42B26D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 285, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23612, ip=127.0.0.1, actor_id=cdfe7c7c7c74613bc2da878901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000218A42B26D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "quant.scale", "quant.zero_point", "conv1.scale", "conv1.zero_point", "conv2.scale", "conv2.zero_point", "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 
	While copying the parameter named "conv1.weight", whose dimensions in the model are torch.Size([6, 3, 5, 5]) and whose dimensions in the checkpoint are torch.Size([6, 3, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).
	While copying the parameter named "conv2.weight", whose dimensions in the model are torch.Size([16, 6, 5, 5]) and whose dimensions in the checkpoint are torch.Size([16, 6, 5, 5]), an exception occurred : ('Copying from quantized Tensor to non-quantized Tensor is not allowed, please use dequantize to get a float Tensor from a quantized Tensor',).

INFO:flwr:aggregate_fit: received 0 results and 10 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 400, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 1712049169367453982
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23043064513206482, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.18205059169530868, {'accuracy': 0.3987}, 32.33000750000065)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=22364, ip=127.0.0.1, actor_id=d5ef832ae14182a8060b15d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002BB66F9A210>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 289, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22364, ip=127.0.0.1, actor_id=d5ef832ae14182a8060b15d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002BB66F9A210>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=22364, ip=127.0.0.1, actor_id=d5ef832ae14182a8060b15d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002BB66F9A210>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 289, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22364, ip=127.0.0.1, actor_id=d5ef832ae14182a8060b15d401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002BB66F9A210>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21448, ip=127.0.0.1, actor_id=3a3010d4ac17959444c093bf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B0E3E7A290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 289, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21448, ip=127.0.0.1, actor_id=3a3010d4ac17959444c093bf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B0E3E7A290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21448, ip=127.0.0.1, actor_id=3a3010d4ac17959444c093bf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B0E3E7A290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 289, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 217, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21448, ip=127.0.0.1, actor_id=3a3010d4ac17959444c093bf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B0E3E7A290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".

INFO:flwr:aggregate_evaluate: received 0 results and 2 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 1 round(s) in 38.06s
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.23043064513206482
INFO:flwr:		round 1: 0.18205059169530868
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1), (1, 0.3987)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 15916297160962839283
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.230410693359375, {'accuracy': 0.1178}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.19439586604833603, {'accuracy': 0.3362}, 30.782451699997182)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27176, ip=127.0.0.1, actor_id=a6482da1bf562eaf99fc645b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000023396042190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 295, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 223, in __init__
    self.client_model.load_state_dict(temp_dequatized_model.state_dict())
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27176, ip=127.0.0.1, actor_id=a6482da1bf562eaf99fc645b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000023396042190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27176, ip=127.0.0.1, actor_id=a6482da1bf562eaf99fc645b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000023396042190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 295, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 223, in __init__
    self.client_model.load_state_dict(temp_dequatized_model.state_dict())
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27176, ip=127.0.0.1, actor_id=a6482da1bf562eaf99fc645b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000023396042190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=25560, ip=127.0.0.1, actor_id=6fdb88db5e0a7db207069d8401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000013C3C51A110>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 295, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 223, in __init__
    self.client_model.load_state_dict(temp_dequatized_model.state_dict())
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=25560, ip=127.0.0.1, actor_id=6fdb88db5e0a7db207069d8401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000013C3C51A110>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=25560, ip=127.0.0.1, actor_id=6fdb88db5e0a7db207069d8401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000013C3C51A110>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 295, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 223, in __init__
    self.client_model.load_state_dict(temp_dequatized_model.state_dict())
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=25560, ip=127.0.0.1, actor_id=6fdb88db5e0a7db207069d8401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000013C3C51A110>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".

INFO:flwr:aggregate_evaluate: received 0 results and 2 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 1 round(s) in 39.52s
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.230410693359375
INFO:flwr:		round 1: 0.19439586604833603
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1178), (1, 0.3362)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 4110384392916300760
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23045001282691954, {'accuracy': 0.0983}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.19915087484121322, {'accuracy': 0.3312}, 29.080227600003127)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 1 round(s) in 36.02s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.15960194638371467
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.23045001282691954
INFO:flwr:		round 1: 0.19915087484121322
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.41574999999999995)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.0983), (1, 0.3312)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 10445158132498318025
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23025566005706788, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.1997663036584854, {'accuracy': 0.3002}, 32.014638099994045)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=5600, ip=127.0.0.1, actor_id=cd8763eba48421ff8c9fe17c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000191F93C2190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=5600, ip=127.0.0.1, actor_id=cd8763eba48421ff8c9fe17c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000191F93C2190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=5600, ip=127.0.0.1, actor_id=cd8763eba48421ff8c9fe17c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000191F93C2190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=5600, ip=127.0.0.1, actor_id=cd8763eba48421ff8c9fe17c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000191F93C2190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=10124, ip=127.0.0.1, actor_id=ff5afd73bcd09dec5f039a2c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B0F8929FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=10124, ip=127.0.0.1, actor_id=ff5afd73bcd09dec5f039a2c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B0F8929FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=10124, ip=127.0.0.1, actor_id=ff5afd73bcd09dec5f039a2c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B0F8929FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=10124, ip=127.0.0.1, actor_id=ff5afd73bcd09dec5f039a2c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B0F8929FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=23016, ip=127.0.0.1, actor_id=62df9628bbb1b7b3e419a90401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DCC2F97E90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23016, ip=127.0.0.1, actor_id=62df9628bbb1b7b3e419a90401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DCC2F97E90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=23016, ip=127.0.0.1, actor_id=62df9628bbb1b7b3e419a90401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DCC2F97E90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23016, ip=127.0.0.1, actor_id=62df9628bbb1b7b3e419a90401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DCC2F97E90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=1164, ip=127.0.0.1, actor_id=21364048cdcffeff1d08489c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025B5FC1A210>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1164, ip=127.0.0.1, actor_id=21364048cdcffeff1d08489c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025B5FC1A210>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=1164, ip=127.0.0.1, actor_id=21364048cdcffeff1d08489c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025B5FC1A210>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1164, ip=127.0.0.1, actor_id=21364048cdcffeff1d08489c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025B5FC1A210>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=984, ip=127.0.0.1, actor_id=f543d72b28970fa92a7dc51a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D2124C3F50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=984, ip=127.0.0.1, actor_id=f543d72b28970fa92a7dc51a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D2124C3F50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=984, ip=127.0.0.1, actor_id=f543d72b28970fa92a7dc51a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D2124C3F50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=984, ip=127.0.0.1, actor_id=f543d72b28970fa92a7dc51a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D2124C3F50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 408, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 9509714096175050643
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23056677174568177, {'accuracy': 0.1008}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=18584, ip=127.0.0.1, actor_id=855d436bccd2d6d042dde02f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021854F41D90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=False)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18584, ip=127.0.0.1, actor_id=855d436bccd2d6d042dde02f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021854F41D90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=18584, ip=127.0.0.1, actor_id=855d436bccd2d6d042dde02f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021854F41D90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=False)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18584, ip=127.0.0.1, actor_id=855d436bccd2d6d042dde02f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021854F41D90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=20468, ip=127.0.0.1, actor_id=871177c49bc65bf3c7560ea401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C775642390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=False)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20468, ip=127.0.0.1, actor_id=871177c49bc65bf3c7560ea401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C775642390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=20468, ip=127.0.0.1, actor_id=871177c49bc65bf3c7560ea401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C775642390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=False)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20468, ip=127.0.0.1, actor_id=871177c49bc65bf3c7560ea401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C775642390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=11416, ip=127.0.0.1, actor_id=898a9e4b9656cbff3061d2bf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019C3FC03050>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=False)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=11416, ip=127.0.0.1, actor_id=898a9e4b9656cbff3061d2bf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019C3FC03050>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=11416, ip=127.0.0.1, actor_id=898a9e4b9656cbff3061d2bf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019C3FC03050>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=False)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=11416, ip=127.0.0.1, actor_id=898a9e4b9656cbff3061d2bf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019C3FC03050>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21812, ip=127.0.0.1, actor_id=5a6a8ebbe29b7a6ab19eb8b401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000258A130A690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=False)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21812, ip=127.0.0.1, actor_id=5a6a8ebbe29b7a6ab19eb8b401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000258A130A690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21812, ip=127.0.0.1, actor_id=5a6a8ebbe29b7a6ab19eb8b401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000258A130A690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=False)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21812, ip=127.0.0.1, actor_id=5a6a8ebbe29b7a6ab19eb8b401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000258A130A690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=16936, ip=127.0.0.1, actor_id=f18ae16bc5a973adcad5b79f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000208A6EC4E90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=False)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16936, ip=127.0.0.1, actor_id=f18ae16bc5a973adcad5b79f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000208A6EC4E90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=16936, ip=127.0.0.1, actor_id=f18ae16bc5a973adcad5b79f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000208A6EC4E90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=False)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16936, ip=127.0.0.1, actor_id=f18ae16bc5a973adcad5b79f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000208A6EC4E90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: division by zero
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 411, in aggregate_fit
    avg_acc = sum(accuracy_list) / len(accuracy_list)
              ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~
ZeroDivisionError: division by zero

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
WARNING:datasets.load:Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub
WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'plain_text' at C:\Users\mathe\.cache\huggingface\datasets\uoft-cs___cifar10\plain_text\0.0.0\0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Sun Mar 16 15:19:12 2025).
DEBUG:flwr:Pre-registering run with id 3485186201161979039
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23050886507034302, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21420, ip=127.0.0.1, actor_id=d3d1958fbebf1fa095a7d31d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A03A019AD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21420, ip=127.0.0.1, actor_id=d3d1958fbebf1fa095a7d31d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A03A019AD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21420, ip=127.0.0.1, actor_id=d3d1958fbebf1fa095a7d31d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A03A019AD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21420, ip=127.0.0.1, actor_id=d3d1958fbebf1fa095a7d31d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A03A019AD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=23228, ip=127.0.0.1, actor_id=8e7d10855e111f4ac07513e801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FC5124BC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23228, ip=127.0.0.1, actor_id=8e7d10855e111f4ac07513e801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FC5124BC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=23228, ip=127.0.0.1, actor_id=8e7d10855e111f4ac07513e801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FC5124BC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23228, ip=127.0.0.1, actor_id=8e7d10855e111f4ac07513e801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FC5124BC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=18700, ip=127.0.0.1, actor_id=ca56e1fe8818bc336f0da4c901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000178455FA690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18700, ip=127.0.0.1, actor_id=ca56e1fe8818bc336f0da4c901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000178455FA690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=31252, ip=127.0.0.1, actor_id=1e6032d24a96098d991d2ce901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000233816A2390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=31252, ip=127.0.0.1, actor_id=1e6032d24a96098d991d2ce901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000233816A2390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=13136, ip=127.0.0.1, actor_id=33a6e6b66ac3fafdadc1a42801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A42A5B9FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13136, ip=127.0.0.1, actor_id=33a6e6b66ac3fafdadc1a42801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A42A5B9FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=18700, ip=127.0.0.1, actor_id=ca56e1fe8818bc336f0da4c901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000178455FA690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18700, ip=127.0.0.1, actor_id=ca56e1fe8818bc336f0da4c901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000178455FA690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=31252, ip=127.0.0.1, actor_id=1e6032d24a96098d991d2ce901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000233816A2390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=31252, ip=127.0.0.1, actor_id=1e6032d24a96098d991d2ce901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000233816A2390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=13136, ip=127.0.0.1, actor_id=33a6e6b66ac3fafdadc1a42801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A42A5B9FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13136, ip=127.0.0.1, actor_id=33a6e6b66ac3fafdadc1a42801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002A42A5B9FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 408, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 17480228850972079403
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23075730385780335, {'accuracy': 0.0978}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.1994193880558014, {'accuracy': 0.3435}, 35.177907300007064)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=15616, ip=127.0.0.1, actor_id=f30fad2f5a6d6d3afbcf717001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B686019310>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=15616, ip=127.0.0.1, actor_id=f30fad2f5a6d6d3afbcf717001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B686019310>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=15616, ip=127.0.0.1, actor_id=f30fad2f5a6d6d3afbcf717001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B686019310>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=15616, ip=127.0.0.1, actor_id=f30fad2f5a6d6d3afbcf717001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001B686019310>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27868, ip=127.0.0.1, actor_id=830a71fd291ebe33854c10ea01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002027D242390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27868, ip=127.0.0.1, actor_id=830a71fd291ebe33854c10ea01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002027D242390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27868, ip=127.0.0.1, actor_id=830a71fd291ebe33854c10ea01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002027D242390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27868, ip=127.0.0.1, actor_id=830a71fd291ebe33854c10ea01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002027D242390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=25188, ip=127.0.0.1, actor_id=1b010a9f2dae36f3353e277401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000279F3973A50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=25188, ip=127.0.0.1, actor_id=1b010a9f2dae36f3353e277401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000279F3973A50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=25188, ip=127.0.0.1, actor_id=1b010a9f2dae36f3353e277401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000279F3973A50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=25188, ip=127.0.0.1, actor_id=1b010a9f2dae36f3353e277401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000279F3973A50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21264, ip=127.0.0.1, actor_id=1c41223c8c1bb4ecb7e6819c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000209E0D12390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21264, ip=127.0.0.1, actor_id=1c41223c8c1bb4ecb7e6819c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000209E0D12390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21264, ip=127.0.0.1, actor_id=1c41223c8c1bb4ecb7e6819c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000209E0D12390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21264, ip=127.0.0.1, actor_id=1c41223c8c1bb4ecb7e6819c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000209E0D12390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=16736, ip=127.0.0.1, actor_id=365915def840cd3e4e7e7fce01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022119BDBC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16736, ip=127.0.0.1, actor_id=365915def840cd3e4e7e7fce01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022119BDBC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=16736, ip=127.0.0.1, actor_id=365915def840cd3e4e7e7fce01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022119BDBC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16736, ip=127.0.0.1, actor_id=365915def840cd3e4e7e7fce01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022119BDBC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 408, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 11561536529562579289
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23078801610469818, {'accuracy': 0.098}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=22348, ip=127.0.0.1, actor_id=057b36b56d0cf7883e15286201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018B55D87D10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22348, ip=127.0.0.1, actor_id=057b36b56d0cf7883e15286201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018B55D87D10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=22348, ip=127.0.0.1, actor_id=057b36b56d0cf7883e15286201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018B55D87D10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22348, ip=127.0.0.1, actor_id=057b36b56d0cf7883e15286201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018B55D87D10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=11172, ip=127.0.0.1, actor_id=ff37cbbc658b7d916192338c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001EF11DB1A10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=11172, ip=127.0.0.1, actor_id=ff37cbbc658b7d916192338c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001EF11DB1A10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=11172, ip=127.0.0.1, actor_id=ff37cbbc658b7d916192338c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001EF11DB1A10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=11172, ip=127.0.0.1, actor_id=ff37cbbc658b7d916192338c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001EF11DB1A10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=32716, ip=127.0.0.1, actor_id=a128ee472a282c4566aa57a701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000151B42D1FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32716, ip=127.0.0.1, actor_id=a128ee472a282c4566aa57a701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000151B42D1FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=32716, ip=127.0.0.1, actor_id=a128ee472a282c4566aa57a701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000151B42D1FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32716, ip=127.0.0.1, actor_id=a128ee472a282c4566aa57a701000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000151B42D1FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=13112, ip=127.0.0.1, actor_id=d8ad25e90479a8ac4023071001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000272EF979AD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13112, ip=127.0.0.1, actor_id=d8ad25e90479a8ac4023071001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000272EF979AD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=13112, ip=127.0.0.1, actor_id=d8ad25e90479a8ac4023071001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000272EF979AD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13112, ip=127.0.0.1, actor_id=d8ad25e90479a8ac4023071001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000272EF979AD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=4076, ip=127.0.0.1, actor_id=a2c2c8860067520eca5c59dd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000028BF6A7BC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=4076, ip=127.0.0.1, actor_id=a2c2c8860067520eca5c59dd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000028BF6A7BC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=4076, ip=127.0.0.1, actor_id=a2c2c8860067520eca5c59dd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000028BF6A7BC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=4076, ip=127.0.0.1, actor_id=a2c2c8860067520eca5c59dd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000028BF6A7BC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 410, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 17692790235797326517
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23040186419487, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=1372, ip=127.0.0.1, actor_id=3e4424968c9d786a5e74db0601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026D7B57A390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1372, ip=127.0.0.1, actor_id=3e4424968c9d786a5e74db0601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026D7B57A390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=1372, ip=127.0.0.1, actor_id=3e4424968c9d786a5e74db0601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026D7B57A390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=1372, ip=127.0.0.1, actor_id=3e4424968c9d786a5e74db0601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026D7B57A390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=4176, ip=127.0.0.1, actor_id=52c699de4bd797829b3ec92001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027B7605A690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=4176, ip=127.0.0.1, actor_id=52c699de4bd797829b3ec92001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027B7605A690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=4176, ip=127.0.0.1, actor_id=52c699de4bd797829b3ec92001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027B7605A690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=4176, ip=127.0.0.1, actor_id=52c699de4bd797829b3ec92001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027B7605A690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=9076, ip=127.0.0.1, actor_id=047c934c092967d3a7f3104001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025F54D9B0D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9076, ip=127.0.0.1, actor_id=047c934c092967d3a7f3104001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025F54D9B0D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=9076, ip=127.0.0.1, actor_id=047c934c092967d3a7f3104001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025F54D9B0D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9076, ip=127.0.0.1, actor_id=047c934c092967d3a7f3104001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025F54D9B0D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=7608, ip=127.0.0.1, actor_id=87aebc9d552014da1e41f21b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001CB10C51FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=7608, ip=127.0.0.1, actor_id=87aebc9d552014da1e41f21b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001CB10C51FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=7608, ip=127.0.0.1, actor_id=87aebc9d552014da1e41f21b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001CB10C51FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=7608, ip=127.0.0.1, actor_id=87aebc9d552014da1e41f21b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001CB10C51FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=30828, ip=127.0.0.1, actor_id=258fc8720caa3cd7ec0a7f3d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000164CC03A690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30828, ip=127.0.0.1, actor_id=258fc8720caa3cd7ec0a7f3d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000164CC03A690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=30828, ip=127.0.0.1, actor_id=258fc8720caa3cd7ec0a7f3d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000164CC03A690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30828, ip=127.0.0.1, actor_id=258fc8720caa3cd7ec0a7f3d01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000164CC03A690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 410, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 5112951814245593884
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.2303280422925949, {'accuracy': 0.0913}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.19999293932914733, {'accuracy': 0.326}, 31.9326582000067)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=32432, ip=127.0.0.1, actor_id=fa10993b5318eb23c942387201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000016E83E52290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32432, ip=127.0.0.1, actor_id=fa10993b5318eb23c942387201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000016E83E52290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=32432, ip=127.0.0.1, actor_id=fa10993b5318eb23c942387201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000016E83E52290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32432, ip=127.0.0.1, actor_id=fa10993b5318eb23c942387201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000016E83E52290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=15712, ip=127.0.0.1, actor_id=6235f74c969104c5375ee1bc01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026B6A9673D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=15712, ip=127.0.0.1, actor_id=6235f74c969104c5375ee1bc01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026B6A9673D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=15712, ip=127.0.0.1, actor_id=6235f74c969104c5375ee1bc01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026B6A9673D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=15712, ip=127.0.0.1, actor_id=6235f74c969104c5375ee1bc01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026B6A9673D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=24392, ip=127.0.0.1, actor_id=ed61dd3d9be39ec70d9991c001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026F9FF44D50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24392, ip=127.0.0.1, actor_id=ed61dd3d9be39ec70d9991c001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026F9FF44D50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=24392, ip=127.0.0.1, actor_id=ed61dd3d9be39ec70d9991c001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026F9FF44D50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24392, ip=127.0.0.1, actor_id=ed61dd3d9be39ec70d9991c001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000026F9FF44D50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=16472, ip=127.0.0.1, actor_id=dbc6d7e0f409cce8e0b42ce001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F084FDA690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16472, ip=127.0.0.1, actor_id=dbc6d7e0f409cce8e0b42ce001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F084FDA690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=16472, ip=127.0.0.1, actor_id=dbc6d7e0f409cce8e0b42ce001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F084FDA690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16472, ip=127.0.0.1, actor_id=dbc6d7e0f409cce8e0b42ce001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F084FDA690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=6520, ip=127.0.0.1, actor_id=2fe15e866fb944a7320c398f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021703977090>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=6520, ip=127.0.0.1, actor_id=2fe15e866fb944a7320c398f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021703977090>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=6520, ip=127.0.0.1, actor_id=2fe15e866fb944a7320c398f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021703977090>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=6520, ip=127.0.0.1, actor_id=2fe15e866fb944a7320c398f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021703977090>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 410, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 13510010893376719554
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23047305262088777, {'accuracy': 0.095}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=20568, ip=127.0.0.1, actor_id=e41809f6cf722f30f2c8410501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022615BE1190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20568, ip=127.0.0.1, actor_id=e41809f6cf722f30f2c8410501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022615BE1190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=20568, ip=127.0.0.1, actor_id=e41809f6cf722f30f2c8410501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022615BE1190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20568, ip=127.0.0.1, actor_id=e41809f6cf722f30f2c8410501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022615BE1190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=23316, ip=127.0.0.1, actor_id=ec668f85ab280e5ae90a7e5a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002528789D5D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23316, ip=127.0.0.1, actor_id=ec668f85ab280e5ae90a7e5a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002528789D5D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=23316, ip=127.0.0.1, actor_id=ec668f85ab280e5ae90a7e5a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002528789D5D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23316, ip=127.0.0.1, actor_id=ec668f85ab280e5ae90a7e5a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002528789D5D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=27460, ip=127.0.0.1, actor_id=3e90fe51c81ebc62b84d7bf001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A808ADA690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27460, ip=127.0.0.1, actor_id=3e90fe51c81ebc62b84d7bf001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A808ADA690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=27460, ip=127.0.0.1, actor_id=3e90fe51c81ebc62b84d7bf001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A808ADA690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=27460, ip=127.0.0.1, actor_id=3e90fe51c81ebc62b84d7bf001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A808ADA690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=9344, ip=127.0.0.1, actor_id=63544dd416184cfa6e9dda5801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022FFDA2DB50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9344, ip=127.0.0.1, actor_id=63544dd416184cfa6e9dda5801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022FFDA2DB50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=9344, ip=127.0.0.1, actor_id=63544dd416184cfa6e9dda5801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022FFDA2DB50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9344, ip=127.0.0.1, actor_id=63544dd416184cfa6e9dda5801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022FFDA2DB50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21148, ip=127.0.0.1, actor_id=8969aa2c05149b981486c00601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027339177D10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21148, ip=127.0.0.1, actor_id=8969aa2c05149b981486c00601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027339177D10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21148, ip=127.0.0.1, actor_id=8969aa2c05149b981486c00601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027339177D10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21148, ip=127.0.0.1, actor_id=8969aa2c05149b981486c00601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027339177D10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 410, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 14710134370649613855
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23049299380779267, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
               ^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\http\client.py", line 1395, in getresponse
    response.begin()
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\http\client.py", line 325, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\http\client.py", line 286, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\ssl.py", line 1314, in recv_into
    return self.read(nbytes, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\ssl.py", line 1166, in read
    return self._sslobj.read(len, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TimeoutError: The read operation timed out

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
  File "D:\work\BTP\venv\Lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\util\retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\util\util.py", line 39, in reraise
    raise value
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\connectionpool.py", line 536, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\connectionpool.py", line 367, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 58, in run
    out_message = app(message=message, context=context)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 294, in client_fn
    trainloader, valloader, _ = load_datasets(partition_id)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 73, in load_datasets
    partition = fds.load_partition(partition_id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr_datasets\federated_dataset.py", line 177, in load_partition
    self._prepare_dataset()
  File "D:\work\BTP\venv\Lib\site-packages\flwr_datasets\federated_dataset.py", line 314, in _prepare_dataset
    self._dataset = datasets.load_dataset(
                    ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\load.py", line 2132, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\load.py", line 1890, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\builder.py", line 342, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\builder.py", line 597, in _create_builder_config
    builder_config._resolve_data_files(
  File "D:\work\BTP\venv\Lib\site-packages\datasets\builder.py", line 206, in _resolve_data_files
    self.data_files = self.data_files.resolve(base_path, download_config)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\data_files.py", line 818, in resolve
    out[key] = data_files_patterns_list.resolve(base_path, download_config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\data_files.py", line 781, in resolve
    origin_metadata = _get_origin_metadata(data_files, download_config=download_config)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\data_files.py", line 548, in _get_origin_metadata
    return thread_map(
           ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\tqdm\contrib\concurrent.py", line 69, in thread_map
    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\tqdm\contrib\concurrent.py", line 51, in _executor_map
    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\tqdm\std.py", line 1169, in __iter__
    for obj in iterable:
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\concurrent\futures\_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\concurrent\futures\_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\data_files.py", line 527, in _get_single_origin_metadata
    resolved_path = fs.resolve_path(data_file)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\hf_file_system.py", line 198, in resolve_path
    repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\hf_file_system.py", line 125, in _repo_and_revision_exist
    self._api.repo_info(
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\hf_api.py", line 2704, in repo_info
    return method(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\hf_api.py", line 2561, in dataset_info
    r = get_session().get(path, headers=headers, timeout=timeout, params=params)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\requests\sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\utils\_http.py", line 93, in send
    return super().send(request, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\requests\adapters.py", line 713, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: a819cb57-96b1-4cb6-b844-14c4288b3af5)')

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: a819cb57-96b1-4cb6-b844-14c4288b3af5)')
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
               ^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\http\client.py", line 1395, in getresponse
    response.begin()
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\http\client.py", line 325, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\http\client.py", line 286, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), "iso-8859-1")
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\ssl.py", line 1314, in recv_into
    return self.read(nbytes, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\ssl.py", line 1166, in read
    return self._sslobj.read(len, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TimeoutError: The read operation timed out

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
  File "D:\work\BTP\venv\Lib\site-packages\requests\adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\util\retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\util\util.py", line 39, in reraise
    raise value
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\connectionpool.py", line 536, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File "D:\work\BTP\venv\Lib\site-packages\urllib3\connectionpool.py", line 367, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)

During handling of the above exception, another exception occurred:

[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 58, in run
    out_message = app(message=message, context=context)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 294, in client_fn
    trainloader, valloader, _ = load_datasets(partition_id)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 73, in load_datasets
    partition = fds.load_partition(partition_id)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr_datasets\federated_dataset.py", line 177, in load_partition
    self._prepare_dataset()
  File "D:\work\BTP\venv\Lib\site-packages\flwr_datasets\federated_dataset.py", line 314, in _prepare_dataset
    self._dataset = datasets.load_dataset(
                    ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\load.py", line 2132, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\load.py", line 1890, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\builder.py", line 342, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\builder.py", line 597, in _create_builder_config
    builder_config._resolve_data_files(
  File "D:\work\BTP\venv\Lib\site-packages\datasets\builder.py", line 206, in _resolve_data_files
    self.data_files = self.data_files.resolve(base_path, download_config)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\data_files.py", line 818, in resolve
    out[key] = data_files_patterns_list.resolve(base_path, download_config)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\data_files.py", line 781, in resolve
    origin_metadata = _get_origin_metadata(data_files, download_config=download_config)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\data_files.py", line 548, in _get_origin_metadata
    return thread_map(
           ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\tqdm\contrib\concurrent.py", line 69, in thread_map
    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\tqdm\contrib\concurrent.py", line 51, in _executor_map
    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\tqdm\std.py", line 1169, in __iter__
    for obj in iterable:
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\concurrent\futures\_base.py", line 619, in result_iterator
    yield _result_or_cancel(fs.pop())
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\concurrent\futures\_base.py", line 317, in _result_or_cancel
    return fut.result(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\concurrent\futures\_base.py", line 456, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\Lib\concurrent\futures\thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\datasets\data_files.py", line 527, in _get_single_origin_metadata
    resolved_path = fs.resolve_path(data_file)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\hf_file_system.py", line 198, in resolve_path
    repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\hf_file_system.py", line 125, in _repo_and_revision_exist
    self._api.repo_info(
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\hf_api.py", line 2704, in repo_info
    return method(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\hf_api.py", line 2561, in dataset_info
    r = get_session().get(path, headers=headers, timeout=timeout, params=params)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\requests\sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\requests\sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\requests\sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\huggingface_hub\utils\_http.py", line 93, in send
    return super().send(request, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\requests\adapters.py", line 713, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: a819cb57-96b1-4cb6-b844-14c4288b3af5)')

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: (ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: a819cb57-96b1-4cb6-b844-14c4288b3af5)')

INFO:flwr:aggregate_fit: received 4 results and 1 failures
INFO:flwr:fit progress: (1, 0.20189990150928497, {'accuracy': 0.3291}, 27.007990400015842)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=25188, ip=127.0.0.1, actor_id=a73a9763c7b875df34b15e8201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C5223BBC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=25188, ip=127.0.0.1, actor_id=a73a9763c7b875df34b15e8201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C5223BBC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=25188, ip=127.0.0.1, actor_id=a73a9763c7b875df34b15e8201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C5223BBC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=25188, ip=127.0.0.1, actor_id=a73a9763c7b875df34b15e8201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C5223BBC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=32672, ip=127.0.0.1, actor_id=032962799dfd08bfa86cff6501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ADE8DE26D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32672, ip=127.0.0.1, actor_id=032962799dfd08bfa86cff6501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ADE8DE26D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=32672, ip=127.0.0.1, actor_id=032962799dfd08bfa86cff6501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ADE8DE26D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32672, ip=127.0.0.1, actor_id=032962799dfd08bfa86cff6501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ADE8DE26D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21464, ip=127.0.0.1, actor_id=5b9557e956b145792f53a4f101000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000022C9839A510>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=26148, ip=127.0.0.1, actor_id=7946adc9853e94d211c35e9301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002067365F3D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=26148, ip=127.0.0.1, actor_id=7946adc9853e94d211c35e9301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002067365F3D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=26148, ip=127.0.0.1, actor_id=7946adc9853e94d211c35e9301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002067365F3D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=26148, ip=127.0.0.1, actor_id=7946adc9853e94d211c35e9301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002067365F3D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

INFO:flwr:aggregate_fit: received 1 results and 4 failures
INFO:flwr:fit progress: (2, 0.1535598391711712, {'accuracy': 0.434}, 51.299522900022566)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 2 round(s) in 56.93s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.19114089065790174
INFO:flwr:		round 2: 0.15455259783566
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.23049299380779267
INFO:flwr:		round 1: 0.20189990150928497
INFO:flwr:		round 2: 0.1535598391711712
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.273), (2, 0.43474999999999997)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1), (1, 0.3291), (2, 0.434)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 7545173113024794049
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23043786580562592, {'accuracy': 0.1021}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.1967319035768509, {'accuracy': 0.3351}, 34.362662600004114)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=34512, ip=127.0.0.1, actor_id=bde2e13c68cd5ee8547f58fe01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ACF0F9B490>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=34512, ip=127.0.0.1, actor_id=bde2e13c68cd5ee8547f58fe01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ACF0F9B490>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=34512, ip=127.0.0.1, actor_id=bde2e13c68cd5ee8547f58fe01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ACF0F9B490>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=34512, ip=127.0.0.1, actor_id=bde2e13c68cd5ee8547f58fe01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001ACF0F9B490>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=29204, ip=127.0.0.1, actor_id=a056ac5e11e412129704d12801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000013ED2DB5910>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29204, ip=127.0.0.1, actor_id=a056ac5e11e412129704d12801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000013ED2DB5910>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=29204, ip=127.0.0.1, actor_id=a056ac5e11e412129704d12801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000013ED2DB5910>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29204, ip=127.0.0.1, actor_id=a056ac5e11e412129704d12801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000013ED2DB5910>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=29780, ip=127.0.0.1, actor_id=f15928f48901249fcea38bb601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C816E94150>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29780, ip=127.0.0.1, actor_id=f15928f48901249fcea38bb601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C816E94150>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=29780, ip=127.0.0.1, actor_id=f15928f48901249fcea38bb601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C816E94150>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29780, ip=127.0.0.1, actor_id=f15928f48901249fcea38bb601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C816E94150>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=28488, ip=127.0.0.1, actor_id=f100a5b53e044dd8f1bd2caf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DC3AF78ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=28488, ip=127.0.0.1, actor_id=f100a5b53e044dd8f1bd2caf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DC3AF78ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=28488, ip=127.0.0.1, actor_id=f100a5b53e044dd8f1bd2caf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DC3AF78ED0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=28488, ip=127.0.0.1, actor_id=f100a5b53e044dd8f1bd2caf01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DC3AF78ED0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=24456, ip=127.0.0.1, actor_id=d50c0640c2b52f15805925c301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025BF6F9A6D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24456, ip=127.0.0.1, actor_id=d50c0640c2b52f15805925c301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025BF6F9A6D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=24456, ip=127.0.0.1, actor_id=d50c0640c2b52f15805925c301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025BF6F9A6D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 245, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24456, ip=127.0.0.1, actor_id=d50c0640c2b52f15805925c301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025BF6F9A6D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 408, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
WARNING:datasets.load:Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub
WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'plain_text' at C:\Users\mathe\.cache\huggingface\datasets\uoft-cs___cifar10\plain_text\0.0.0\0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Sun Mar 16 15:19:12 2025).
DEBUG:flwr:Pre-registering run with id 752087012794216871
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23044152023792266, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=30752, ip=127.0.0.1, actor_id=edc1b0f7a245f7deb757d83301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002D671A7B490>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30752, ip=127.0.0.1, actor_id=edc1b0f7a245f7deb757d83301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002D671A7B490>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=30752, ip=127.0.0.1, actor_id=edc1b0f7a245f7deb757d83301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002D671A7B490>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30752, ip=127.0.0.1, actor_id=edc1b0f7a245f7deb757d83301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002D671A7B490>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=33240, ip=127.0.0.1, actor_id=78dd3d1e7a1f98ecbde0572501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F1BFBE55D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=33240, ip=127.0.0.1, actor_id=78dd3d1e7a1f98ecbde0572501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F1BFBE55D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=33240, ip=127.0.0.1, actor_id=78dd3d1e7a1f98ecbde0572501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F1BFBE55D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=33240, ip=127.0.0.1, actor_id=78dd3d1e7a1f98ecbde0572501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F1BFBE55D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=34076, ip=127.0.0.1, actor_id=22c24fb2eab34b4ae37fa32a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019F80122390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=34076, ip=127.0.0.1, actor_id=22c24fb2eab34b4ae37fa32a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019F80122390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=34076, ip=127.0.0.1, actor_id=22c24fb2eab34b4ae37fa32a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019F80122390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=34076, ip=127.0.0.1, actor_id=22c24fb2eab34b4ae37fa32a01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000019F80122390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=20792, ip=127.0.0.1, actor_id=eaaca2ca05ffb78ac53be0f401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021F853E2390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20792, ip=127.0.0.1, actor_id=eaaca2ca05ffb78ac53be0f401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021F853E2390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=20792, ip=127.0.0.1, actor_id=eaaca2ca05ffb78ac53be0f401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021F853E2390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20792, ip=127.0.0.1, actor_id=eaaca2ca05ffb78ac53be0f401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021F853E2390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=32956, ip=127.0.0.1, actor_id=e9a01e26139b0287ddecb40301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AC6E321FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32956, ip=127.0.0.1, actor_id=e9a01e26139b0287ddecb40301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AC6E321FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=32956, ip=127.0.0.1, actor_id=e9a01e26139b0287ddecb40301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AC6E321FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 247, in fit
    set_parameters(self.client_model, ndarrays_original)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2561, in load_state_dict
    load(self, state_dict)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2549, in load
    load(child, child_state_dict, child_prefix)  # noqa: F821
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2532, in load
    module._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\dynamic\modules\linear.py", line 89, in _load_from_state_dict
    super()._load_from_state_dict(
  File "D:\work\BTP\venv\Lib\site-packages\torch\ao\nn\quantized\modules\linear.py", line 241, in _load_from_state_dict
    self.scale = float(state_dict[prefix + "scale"])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: only one element tensors can be converted to Python scalars

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32956, ip=127.0.0.1, actor_id=e9a01e26139b0287ddecb40301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AC6E321FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: only one element tensors can be converted to Python scalars

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 410, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_quantisation.py", line 136, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
WARNING:datasets.load:Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub
WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'plain_text' at C:\Users\mathe\.cache\huggingface\datasets\uoft-cs___cifar10\plain_text\0.0.0\0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Sun Mar 16 15:19:12 2025).
DEBUG:flwr:Pre-registering run with id 129899295878300164
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23048362667560576, {'accuracy': 0.0982}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=3204, ip=127.0.0.1, actor_id=2c2097f5f9a97c72525b087501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027DC17A4250>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 337, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 266, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=3204, ip=127.0.0.1, actor_id=2c2097f5f9a97c72525b087501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027DC17A4250>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=3204, ip=127.0.0.1, actor_id=2c2097f5f9a97c72525b087501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027DC17A4250>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 337, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 266, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=3204, ip=127.0.0.1, actor_id=2c2097f5f9a97c72525b087501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000027DC17A4250>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=15564, ip=127.0.0.1, actor_id=3aff96ac21f2583ba12e636201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FD9BFE1C90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 337, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 266, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=15564, ip=127.0.0.1, actor_id=3aff96ac21f2583ba12e636201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FD9BFE1C90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=15564, ip=127.0.0.1, actor_id=3aff96ac21f2583ba12e636201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FD9BFE1C90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 337, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 266, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=15564, ip=127.0.0.1, actor_id=3aff96ac21f2583ba12e636201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FD9BFE1C90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=30864, ip=127.0.0.1, actor_id=36b33ceca2e72fa014e4419901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000266264E2190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30864, ip=127.0.0.1, actor_id=36b33ceca2e72fa014e4419901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000266264E2190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=30864, ip=127.0.0.1, actor_id=36b33ceca2e72fa014e4419901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000266264E2190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30864, ip=127.0.0.1, actor_id=36b33ceca2e72fa014e4419901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000266264E2190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=34616, ip=127.0.0.1, actor_id=1a7657d3c9dfcf20d4d14b0201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D696776650>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=34616, ip=127.0.0.1, actor_id=1a7657d3c9dfcf20d4d14b0201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D696776650>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=34616, ip=127.0.0.1, actor_id=1a7657d3c9dfcf20d4d14b0201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D696776650>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=34616, ip=127.0.0.1, actor_id=1a7657d3c9dfcf20d4d14b0201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001D696776650>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=26884, ip=127.0.0.1, actor_id=f45739e30998881134b4719b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002637639AB90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=26884, ip=127.0.0.1, actor_id=f45739e30998881134b4719b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002637639AB90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=26884, ip=127.0.0.1, actor_id=f45739e30998881134b4719b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002637639AB90>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=26884, ip=127.0.0.1, actor_id=f45739e30998881134b4719b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002637639AB90>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=21736, ip=127.0.0.1, actor_id=cac1fc73b03b6e6d03af2b1e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AD46601A10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21736, ip=127.0.0.1, actor_id=cac1fc73b03b6e6d03af2b1e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AD46601A10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=21736, ip=127.0.0.1, actor_id=cac1fc73b03b6e6d03af2b1e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AD46601A10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=21736, ip=127.0.0.1, actor_id=cac1fc73b03b6e6d03af2b1e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AD46601A10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=13628, ip=127.0.0.1, actor_id=b4fb51b1ab769d953616563401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002F1BF9F4D50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 337, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 266, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13628, ip=127.0.0.1, actor_id=b4fb51b1ab769d953616563401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002F1BF9F4D50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=13628, ip=127.0.0.1, actor_id=b4fb51b1ab769d953616563401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002F1BF9F4D50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 337, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 266, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13628, ip=127.0.0.1, actor_id=b4fb51b1ab769d953616563401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002F1BF9F4D50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=24816, ip=127.0.0.1, actor_id=b33be0bf1a19511fab49085801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FC324BBF10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24816, ip=127.0.0.1, actor_id=b33be0bf1a19511fab49085801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FC324BBF10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=24816, ip=127.0.0.1, actor_id=b33be0bf1a19511fab49085801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FC324BBF10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24816, ip=127.0.0.1, actor_id=b33be0bf1a19511fab49085801000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FC324BBF10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=33716, ip=127.0.0.1, actor_id=1c818b46440b91e8a79e56f401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000150CCABDB50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 337, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 266, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=33716, ip=127.0.0.1, actor_id=1c818b46440b91e8a79e56f401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000150CCABDB50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=33716, ip=127.0.0.1, actor_id=1c818b46440b91e8a79e56f401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000150CCABDB50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 96, in handle_legacy_message_from_msgtype
    client = client_fn(context)
             ^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 337, in client_fn
    return FlowerClient(partition_id, net, trainloader, valloader).to_client()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 266, in __init__
    self.client_model.load_state_dict(torch.load(self.model_path))
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params". 

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=33716, ip=127.0.0.1, actor_id=1c818b46440b91e8a79e56f401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000150CCABDB50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
	Unexpected key(s) in state_dict: "fc1.scale", "fc1.zero_point", "fc1._packed_params.dtype", "fc1._packed_params._packed_params", "fc2.scale", "fc2.zero_point", "fc2._packed_params.dtype", "fc2._packed_params._packed_params", "fc3.scale", "fc3.zero_point", "fc3._packed_params.dtype", "fc3._packed_params._packed_params".

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=3288, ip=127.0.0.1, actor_id=5a266cf6ecc6fd30601ce3f501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AD69C9A190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=3288, ip=127.0.0.1, actor_id=5a266cf6ecc6fd30601ce3f501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AD69C9A190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=3288, ip=127.0.0.1, actor_id=5a266cf6ecc6fd30601ce3f501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AD69C9A190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=3288, ip=127.0.0.1, actor_id=5a266cf6ecc6fd30601ce3f501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AD69C9A190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)

INFO:flwr:aggregate_fit: received 0 results and 10 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 452, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 191, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 16984327817344420391
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23031036729812623, {'accuracy': 0.1311}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=14540, ip=127.0.0.1, actor_id=e8d2c8af93b319a475175ffd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C1437DD5D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=14540, ip=127.0.0.1, actor_id=e8d2c8af93b319a475175ffd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C1437DD5D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=14540, ip=127.0.0.1, actor_id=e8d2c8af93b319a475175ffd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C1437DD5D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=14540, ip=127.0.0.1, actor_id=e8d2c8af93b319a475175ffd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001C1437DD5D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=9868, ip=127.0.0.1, actor_id=c91b91dfa8edbba2580c708201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000230C8DFF3D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9868, ip=127.0.0.1, actor_id=c91b91dfa8edbba2580c708201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000230C8DFF3D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=9868, ip=127.0.0.1, actor_id=c91b91dfa8edbba2580c708201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000230C8DFF3D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9868, ip=127.0.0.1, actor_id=c91b91dfa8edbba2580c708201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000230C8DFF3D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=9920, ip=127.0.0.1, actor_id=ddfaf78fb6817fce0d09b7c301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002532617B0D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9920, ip=127.0.0.1, actor_id=ddfaf78fb6817fce0d09b7c301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002532617B0D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=9920, ip=127.0.0.1, actor_id=ddfaf78fb6817fce0d09b7c301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002532617B0D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=9920, ip=127.0.0.1, actor_id=ddfaf78fb6817fce0d09b7c301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002532617B0D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=10496, ip=127.0.0.1, actor_id=6b449f7656c0f5a0c4e45ff901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F892518E50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=10496, ip=127.0.0.1, actor_id=6b449f7656c0f5a0c4e45ff901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F892518E50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=10496, ip=127.0.0.1, actor_id=6b449f7656c0f5a0c4e45ff901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F892518E50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=10496, ip=127.0.0.1, actor_id=6b449f7656c0f5a0c4e45ff901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001F892518E50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=18764, ip=127.0.0.1, actor_id=95d75f788e10ead984e257ee01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000236BE743C50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18764, ip=127.0.0.1, actor_id=95d75f788e10ead984e257ee01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000236BE743C50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=18764, ip=127.0.0.1, actor_id=95d75f788e10ead984e257ee01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000236BE743C50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 299, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([torch.from_numpy(i).flatten().abs() for i in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
TypeError: expected np.ndarray (got Tensor)

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=18764, ip=127.0.0.1, actor_id=95d75f788e10ead984e257ee01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000236BE743C50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: expected np.ndarray (got Tensor)

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 450, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 191, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 14122470302516655546
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23052921648025512, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=22240, ip=127.0.0.1, actor_id=3b88f8293bb002913f45258f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000287BCBE4210>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 301, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22240, ip=127.0.0.1, actor_id=3b88f8293bb002913f45258f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000287BCBE4210>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=22240, ip=127.0.0.1, actor_id=3b88f8293bb002913f45258f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000287BCBE4210>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 301, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=22240, ip=127.0.0.1, actor_id=3b88f8293bb002913f45258f01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000287BCBE4210>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=6200, ip=127.0.0.1, actor_id=9cc76114207065eb6e890afe01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AF80079FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 301, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=6200, ip=127.0.0.1, actor_id=9cc76114207065eb6e890afe01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AF80079FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=6200, ip=127.0.0.1, actor_id=9cc76114207065eb6e890afe01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AF80079FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 301, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=6200, ip=127.0.0.1, actor_id=9cc76114207065eb6e890afe01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001AF80079FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=13200, ip=127.0.0.1, actor_id=530778d9ca0f895823d994f901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018680BE1A10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 301, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13200, ip=127.0.0.1, actor_id=530778d9ca0f895823d994f901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018680BE1A10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=13200, ip=127.0.0.1, actor_id=530778d9ca0f895823d994f901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018680BE1A10>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 301, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13200, ip=127.0.0.1, actor_id=530778d9ca0f895823d994f901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018680BE1A10>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=32000, ip=127.0.0.1, actor_id=ff92d9ba94efc7cba5d0649b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B600041590>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 301, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32000, ip=127.0.0.1, actor_id=ff92d9ba94efc7cba5d0649b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B600041590>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=32000, ip=127.0.0.1, actor_id=ff92d9ba94efc7cba5d0649b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B600041590>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 301, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32000, ip=127.0.0.1, actor_id=ff92d9ba94efc7cba5d0649b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000002B600041590>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=34124, ip=127.0.0.1, actor_id=82261501f077ed86dc87cbb201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021EC4C3DB50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 301, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=34124, ip=127.0.0.1, actor_id=82261501f077ed86dc87cbb201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021EC4C3DB50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=34124, ip=127.0.0.1, actor_id=82261501f077ed86dc87cbb201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021EC4C3DB50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 301, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=34124, ip=127.0.0.1, actor_id=82261501f077ed86dc87cbb201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000021EC4C3DB50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 450, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 191, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 17586284581496729542
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.2304470257997513, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=24380, ip=127.0.0.1, actor_id=6cf549ea4c32a4c9dd1931cd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000020B76439590>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 315, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24380, ip=127.0.0.1, actor_id=6cf549ea4c32a4c9dd1931cd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000020B76439590>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=24380, ip=127.0.0.1, actor_id=6cf549ea4c32a4c9dd1931cd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000020B76439590>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 315, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24380, ip=127.0.0.1, actor_id=6cf549ea4c32a4c9dd1931cd01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000020B76439590>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=29416, ip=127.0.0.1, actor_id=eefc0de4ecebb842bad38d7601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E52EA4A0D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 315, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29416, ip=127.0.0.1, actor_id=eefc0de4ecebb842bad38d7601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E52EA4A0D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=29416, ip=127.0.0.1, actor_id=eefc0de4ecebb842bad38d7601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E52EA4A0D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 315, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=29416, ip=127.0.0.1, actor_id=eefc0de4ecebb842bad38d7601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E52EA4A0D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=4828, ip=127.0.0.1, actor_id=069584938d5dacd53e851d1b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001999FA8BC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 315, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=4828, ip=127.0.0.1, actor_id=069584938d5dacd53e851d1b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001999FA8BC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=4828, ip=127.0.0.1, actor_id=069584938d5dacd53e851d1b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001999FA8BC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 315, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=4828, ip=127.0.0.1, actor_id=069584938d5dacd53e851d1b01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001999FA8BC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=24968, ip=127.0.0.1, actor_id=62bc501d7f2da364dc63eb6001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E55ECB9FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 315, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24968, ip=127.0.0.1, actor_id=62bc501d7f2da364dc63eb6001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E55ECB9FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=24968, ip=127.0.0.1, actor_id=62bc501d7f2da364dc63eb6001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E55ECB9FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 315, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=24968, ip=127.0.0.1, actor_id=62bc501d7f2da364dc63eb6001000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001E55ECB9FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=12816, ip=127.0.0.1, actor_id=cad5edf55bcae2f2fb62089501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FB72F9B0D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 315, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=12816, ip=127.0.0.1, actor_id=cad5edf55bcae2f2fb62089501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FB72F9B0D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=12816, ip=127.0.0.1, actor_id=cad5edf55bcae2f2fb62089501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FB72F9B0D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 315, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 119, in ndarray_to_sparse_bytes
    ndarray = torch.tensor(ndarray).to_sparse_csr()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expect the same number of specified elements per batch.

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=12816, ip=127.0.0.1, actor_id=cad5edf55bcae2f2fb62089501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001FB72F9B0D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: Expect the same number of specified elements per batch.

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 464, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 191, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 9201412924611595436
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23041120843887328, {'accuracy': 0.1001}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
ERROR:flwr:ServerApp thread raised an exception: Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations.
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 512, in aggregate_fit
    new_v += 0.5 * (global_best_params[idx] - param)
TypeError: Concatenation operation is not implemented for NumPy arrays, use np.concatenate() instead. Please do not rely on this error; it may not be given on all Python implementations.

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 12653100589916466016
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23047909202575684, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (1, 0.20832831474542618, {'accuracy': 0.2424}, 25.731431799998973)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 1 round(s) in 44.14s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.17311058166027068
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.23047909202575684
INFO:flwr:		round 1: 0.20832831474542618
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.35519999999999996)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1), (1, 0.2424)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 12968814645029960660
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23038417456150054, {'accuracy': 0.0993}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=32812, ip=127.0.0.1, actor_id=0e916541f2fc32a2eccd687e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025DB6D22210>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 308, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'abs'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32812, ip=127.0.0.1, actor_id=0e916541f2fc32a2eccd687e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025DB6D22210>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'numpy.ndarray' object has no attribute 'abs'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=32812, ip=127.0.0.1, actor_id=0e916541f2fc32a2eccd687e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025DB6D22210>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 308, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'abs'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=32812, ip=127.0.0.1, actor_id=0e916541f2fc32a2eccd687e01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000025DB6D22210>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'numpy.ndarray' object has no attribute 'abs'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=23852, ip=127.0.0.1, actor_id=bfabaf03599d5da5dd12729c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000246C9E21FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 308, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'abs'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23852, ip=127.0.0.1, actor_id=bfabaf03599d5da5dd12729c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000246C9E21FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'numpy.ndarray' object has no attribute 'abs'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=23852, ip=127.0.0.1, actor_id=bfabaf03599d5da5dd12729c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000246C9E21FD0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 308, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'abs'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=23852, ip=127.0.0.1, actor_id=bfabaf03599d5da5dd12729c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000246C9E21FD0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'numpy.ndarray' object has no attribute 'abs'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=17508, ip=127.0.0.1, actor_id=46992d8aec4c4c07cd18927301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000259CE0192D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 308, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'abs'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=17508, ip=127.0.0.1, actor_id=46992d8aec4c4c07cd18927301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000259CE0192D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'numpy.ndarray' object has no attribute 'abs'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=17508, ip=127.0.0.1, actor_id=46992d8aec4c4c07cd18927301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000259CE0192D0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 308, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'abs'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=17508, ip=127.0.0.1, actor_id=46992d8aec4c4c07cd18927301000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000259CE0192D0>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'numpy.ndarray' object has no attribute 'abs'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=4068, ip=127.0.0.1, actor_id=7b541970142c7cee8ccef37c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000023085BBBC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 308, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'abs'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=4068, ip=127.0.0.1, actor_id=7b541970142c7cee8ccef37c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000023085BBBC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'numpy.ndarray' object has no attribute 'abs'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=4068, ip=127.0.0.1, actor_id=7b541970142c7cee8ccef37c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000023085BBBC50>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 308, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'abs'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=4068, ip=127.0.0.1, actor_id=7b541970142c7cee8ccef37c01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000023085BBBC50>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'numpy.ndarray' object has no attribute 'abs'

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=34568, ip=127.0.0.1, actor_id=329c2a5ba273668cf3e1ddab01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018AEF6E2290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 308, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'abs'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=34568, ip=127.0.0.1, actor_id=329c2a5ba273668cf3e1ddab01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018AEF6E2290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'numpy.ndarray' object has no attribute 'abs'
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=34568, ip=127.0.0.1, actor_id=329c2a5ba273668cf3e1ddab01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018AEF6E2290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 308, in fit
    ndarrays_pruned = prune_threshold(ndarrays_updated, 0.5)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in prune_threshold
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 242, in <listcomp>
    sorted = torch.cat([param.flatten().abs() for param in params]).sort()[0]
                        ^^^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'abs'

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=34568, ip=127.0.0.1, actor_id=329c2a5ba273668cf3e1ddab01000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000018AEF6E2290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: 'numpy.ndarray' object has no attribute 'abs'

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 464, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 191, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 712879935624351615
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23043603360652923, {'accuracy': 0.1223}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.18489294302463533, {'accuracy': 0.3657}, 27.52407400001539)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (2, 0.16239858144521713, {'accuracy': 0.4468}, 57.331657900009304)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 2 round(s) in 63.49s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.15905654947459696
INFO:flwr:		round 2: 0.14478078351914883
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.23043603360652923
INFO:flwr:		round 1: 0.18489294302463533
INFO:flwr:		round 2: 0.16239858144521713
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.42925), (2, 0.4835)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1223), (1, 0.3657), (2, 0.4468)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 11658924283996238015
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23051880381107331, {'accuracy': 0.101}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=13628, ip=127.0.0.1, actor_id=251f27f71d8ad05ae8d0ff8201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000011B584E2190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 341, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 154, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13628, ip=127.0.0.1, actor_id=251f27f71d8ad05ae8d0ff8201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000011B584E2190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=13628, ip=127.0.0.1, actor_id=251f27f71d8ad05ae8d0ff8201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000011B584E2190>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 341, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 154, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=13628, ip=127.0.0.1, actor_id=251f27f71d8ad05ae8d0ff8201000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x0000011B584E2190>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=20884, ip=127.0.0.1, actor_id=9a349f39989843cd53bec76501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000257BD6D2290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 341, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 154, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20884, ip=127.0.0.1, actor_id=9a349f39989843cd53bec76501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000257BD6D2290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=20884, ip=127.0.0.1, actor_id=9a349f39989843cd53bec76501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000257BD6D2290>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 341, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 154, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=20884, ip=127.0.0.1, actor_id=9a349f39989843cd53bec76501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000257BD6D2290>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=30508, ip=127.0.0.1, actor_id=b38f17edcdb0b1cfe7ba89f501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DFC2CA1890>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 341, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 154, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30508, ip=127.0.0.1, actor_id=b38f17edcdb0b1cfe7ba89f501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DFC2CA1890>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=30508, ip=127.0.0.1, actor_id=b38f17edcdb0b1cfe7ba89f501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DFC2CA1890>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 341, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 154, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=30508, ip=127.0.0.1, actor_id=b38f17edcdb0b1cfe7ba89f501000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001DFC2CA1890>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=3752, ip=127.0.0.1, actor_id=143b41d40662e71283adab0601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A78DFE2690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 341, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 154, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=3752, ip=127.0.0.1, actor_id=143b41d40662e71283adab0601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A78DFE2690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=3752, ip=127.0.0.1, actor_id=143b41d40662e71283adab0601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A78DFE2690>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 341, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 154, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=3752, ip=127.0.0.1, actor_id=143b41d40662e71283adab0601000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x000001A78DFE2690>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

ERROR:flwr:An exception was raised when processing a message by RayBackend
ERROR:flwr:[36mray::ClientAppActor.run()[39m (pid=16120, ip=127.0.0.1, actor_id=7c97bce2694e82ca2413aab401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000169FCA42390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 341, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 154, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16120, ip=127.0.0.1, actor_id=7c97bce2694e82ca2413aab401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000169FCA42390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 187, in process_message
    raise ex
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\superlink\fleet\vce\backend\raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\worker.py", line 864, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ClientAppException): [36mray::ClientAppActor.run()[39m (pid=16120, ip=127.0.0.1, actor_id=7c97bce2694e82ca2413aab401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000169FCA42390>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 143, in __call__
    return self._call(message, context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client_app.py", line 126, in ffn
    out_message = handle_legacy_message_from_msgtype(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\message_handler\message_handler.py", line 128, in handle_legacy_message_from_msgtype
    fit_res = maybe_call_fit(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\client\client.py", line 224, in maybe_call_fit
    return client.fit(fit_ins)
           ^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 341, in fit
    parameters_updated = ndarrays_to_sparse_parameters(ndarrays_pruned)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in ndarrays_to_sparse_parameters
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 104, in <listcomp>
    tensors = [ndarray_to_sparse_bytes(ndarray) for ndarray in ndarrays]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 154, in ndarray_to_sparse_bytes
    crow_indices=ndarray.crow_indices(),
                 ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: crow_indices expected sparse row compressed tensor layout but got Sparse

The above exception was the direct cause of the following exception:

[36mray::ClientAppActor.run()[39m (pid=16120, ip=127.0.0.1, actor_id=7c97bce2694e82ca2413aab401000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x00000169FCA42390>)
  File "python\ray\_raylet.pyx", line 1883, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1984, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1889, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1830, in ray._raylet.execute_task.function_executor
  File "D:\work\BTP\venv\Lib\site-packages\ray\_private\function_manager.py", line 724, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\ray_transport\ray_actor.py", line 64, in run
    raise ClientAppException(str(ex)) from ex
flwr.client.client_app.ClientAppException: 
Exception ClientAppException occurred. Message: crow_indices expected sparse row compressed tensor layout but got Sparse

INFO:flwr:aggregate_fit: received 0 results and 5 failures
ERROR:flwr:ServerApp thread raised an exception: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 
ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 490, in aggregate_fit
    set_parameters(self.global_model, aggregated_weights)
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 218, in set_parameters
    net.load_state_dict(state_dict, strict=True)
  File "D:\work\BTP\venv\Lib\site-packages\torch\nn\modules\module.py", line 2581, in load_state_dict
    raise RuntimeError(
RuntimeError: Error(s) in loading state_dict for Net:
	Missing key(s) in state_dict: "conv1.weight", "conv1.bias", "conv2.weight", "conv2.bias", "fc1.weight", "fc1.bias", "fc2.weight", "fc2.bias", "fc3.weight", "fc3.bias". 

DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
DEBUG:flwr:Pre-registering run with id 17585459834801963378
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23047603878974915, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
ERROR:flwr:ServerApp thread raised an exception: sparse_coo_tensor() received an invalid combination of arguments - got (indices=numpy.ndarray, values=numpy.ndarray, size=numpy.ndarray, ), but expected one of:
 * (object indices, object values, *, torch.dtype dtype = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False, bool check_invariants = None)
 * (object indices, object values, tuple of ints size, *, torch.dtype dtype = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False, bool check_invariants = None, bool is_coalesced = None)
 * (tuple of ints size, *, torch.dtype dtype = None, torch.device device = None, bool requires_grad = False, bool check_invariants = None)

ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 504, in aggregate_fit
    weights_results = [
                      ^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 505, in <listcomp>
    (sparse_parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 110, in sparse_parameters_to_ndarrays
    return [sparse_bytes_to_ndarray(tensor) for tensor in parameters.tensors]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 110, in <listcomp>
    return [sparse_bytes_to_ndarray(tensor) for tensor in parameters.tensors]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 198, in sparse_bytes_to_ndarray
    torch.sparse_coo_tensor(
TypeError: sparse_coo_tensor() received an invalid combination of arguments - got (indices=numpy.ndarray, values=numpy.ndarray, size=numpy.ndarray, ), but expected one of:
 * (object indices, object values, *, torch.dtype dtype = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False, bool check_invariants = None)
 * (object indices, object values, tuple of ints size, *, torch.dtype dtype = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False, bool check_invariants = None, bool is_coalesced = None)
 * (tuple of ints size, *, torch.dtype dtype = None, torch.device device = None, bool requires_grad = False, bool check_invariants = None)


DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
WARNING:datasets.load:Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub
WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'plain_text' at C:\Users\mathe\.cache\huggingface\datasets\uoft-cs___cifar10\plain_text\0.0.0\0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Sun Mar 16 15:19:12 2025).
DEBUG:flwr:Pre-registering run with id 3759007299750888508
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23045846164226533, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
ERROR:flwr:ServerApp thread raised an exception: sparse_coo_tensor() received an invalid combination of arguments - got (indices=numpy.ndarray, values=numpy.ndarray, size=numpy.ndarray, ), but expected one of:
 * (object indices, object values, *, torch.dtype dtype = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False, bool check_invariants = None)
 * (object indices, object values, tuple of ints size, *, torch.dtype dtype = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False, bool check_invariants = None, bool is_coalesced = None)
 * (tuple of ints size, *, torch.dtype dtype = None, torch.device device = None, bool requires_grad = False, bool check_invariants = None)

ERROR:flwr:Traceback (most recent call last):
  File "D:\work\BTP\venv\Lib\site-packages\flwr\simulation\run_simulation.py", line 268, in server_th_with_start_checks
    updated_context = _run(
                      ^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\run_serverapp.py", line 63, in run
    server_app(driver=driver, context=context)
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server_app.py", line 120, in __call__
    start_driver(
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\compat\app.py", line 87, in start_driver
    hist = run_fl(
           ^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 492, in run_fl
    hist, elapsed_time = server.fit(
                         ^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 115, in fit
    res_fit = self.fit_round(
              ^^^^^^^^^^^^^^^
  File "D:\work\BTP\venv\Lib\site-packages\flwr\server\server.py", line 251, in fit_round
    ] = self.strategy.aggregate_fit(server_round, results, failures)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 504, in aggregate_fit
    weights_results = [
                      ^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 505, in <listcomp>
    (sparse_parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 110, in sparse_parameters_to_ndarrays
    return [sparse_bytes_to_ndarray(tensor) for tensor in parameters.tensors]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 110, in <listcomp>
    return [sparse_bytes_to_ndarray(tensor) for tensor in parameters.tensors]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "d:\work\BTP\FedCPSO\fedcpso_pruning.py", line 198, in sparse_bytes_to_ndarray
    torch.sparse_coo_tensor(
TypeError: sparse_coo_tensor() received an invalid combination of arguments - got (indices=numpy.ndarray, values=numpy.ndarray, size=numpy.ndarray, ), but expected one of:
 * (object indices, object values, *, torch.dtype dtype = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False, bool check_invariants = None)
 * (object indices, object values, tuple of ints size, *, torch.dtype dtype = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False, bool check_invariants = None, bool is_coalesced = None)
 * (tuple of ints size, *, torch.dtype dtype = None, torch.device device = None, bool requires_grad = False, bool check_invariants = None)


DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Queue timeout. No context received.
WARNING:datasets.load:Using the latest cached version of the dataset since uoft-cs/cifar10 couldn't be found on the Hugging Face Hub
WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'plain_text' at C:\Users\mathe\.cache\huggingface\datasets\uoft-cs___cifar10\plain_text\0.0.0\0b2714987fa478483af9968de7c934580d0bb9a2 (last modified on Sun Mar 16 15:19:12 2025).
DEBUG:flwr:Pre-registering run with id 2450010726987090724
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23047541263103485, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.19025256690979003, {'accuracy': 0.3425}, 20.945720599993365)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (2, 0.16402661984562875, {'accuracy': 0.4256}, 46.86499329999788)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 2 round(s) in 47.87s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.15634874626994133
INFO:flwr:		round 2: 0.14356312806904317
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.23047541263103485
INFO:flwr:		round 1: 0.19025256690979003
INFO:flwr:		round 2: 0.16402661984562875
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.431), (2, 0.47575)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1), (1, 0.3425), (2, 0.4256)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 9231363345098087789
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=2, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.2304959257364273, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.19140881099700927, {'accuracy': 0.3545}, 29.839236800005892)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (2, 0.1637311769247055, {'accuracy': 0.4479}, 65.60012990000541)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 2 round(s) in 73.45s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.1545445269793272
INFO:flwr:		round 2: 0.14453895422816276
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.2304959257364273
INFO:flwr:		round 1: 0.19140881099700927
INFO:flwr:		round 2: 0.1637311769247055
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.43725), (2, 0.483)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1), (1, 0.3545), (2, 0.4479)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 11994491622136682276
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 10 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=1, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.2304706674337387, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 10 clients (out of 10)
INFO:flwr:aggregate_fit: received 10 results and 0 failures
INFO:flwr:fit progress: (1, 0.21101318432092667, {'accuracy': 0.2386}, 31.892674600007012)
INFO:flwr:configure_evaluate: strategy sampled 5 clients (out of 10)
INFO:flwr:aggregate_evaluate: received 5 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 1 round(s) in 43.37s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.17311389770507812
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.2304706674337387
INFO:flwr:		round 1: 0.21101318432092667
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.376)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1), (1, 0.2386)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
DEBUG:flwr:Pre-registering run with id 2596260294755147895
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Using InMemoryState
DEBUG:flwr:Registered 5 nodes
DEBUG:flwr:Supported backends: ['ray']
DEBUG:flwr:Initialising: RayBackend
DEBUG:flwr:Backend config: {'client_resources': {'num_cpus': 1, 'num_gpus': 0.0}, 'init_args': {'logging_level': 30, 'log_to_driver': True}, 'actor': {'tensorflow': 0}}
INFO:flwr:Starting Flower ServerApp, config: num_rounds=5, no round_timeout
INFO:flwr:
INFO:flwr:[INIT]
INFO:flwr:Using initial global parameters provided by strategy
INFO:flwr:Starting evaluation of initial global parameters
DEBUG:flwr:Constructed ActorPool with: 16 actors
DEBUG:flwr:Using InMemoryState
INFO:flwr:initial parameters (loss, other metrics): 0.23037902109622954, {'accuracy': 0.1}
INFO:flwr:
INFO:flwr:[ROUND 1]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (1, 0.20249070378541947, {'accuracy': 0.3473}, 27.731662200007122)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 2]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (2, 0.18032368122339248, {'accuracy': 0.4098}, 55.61310070002219)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 3]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (3, 0.1572258873164654, {'accuracy': 0.4599}, 84.82819000002928)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 4]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (4, 0.13654857886433602, {'accuracy': 0.5172}, 115.06073260004632)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[ROUND 5]
INFO:flwr:configure_fit: strategy sampled 5 clients (out of 5)
INFO:flwr:aggregate_fit: received 5 results and 0 failures
INFO:flwr:fit progress: (5, 0.1251989823669195, {'accuracy': 0.5618}, 141.86370310001075)
INFO:flwr:configure_evaluate: strategy sampled 2 clients (out of 5)
INFO:flwr:aggregate_evaluate: received 2 results and 0 failures
INFO:flwr:
INFO:flwr:[SUMMARY]
INFO:flwr:Run finished 5 round(s) in 156.86s
INFO:flwr:	History (loss, distributed):
INFO:flwr:		round 1: 0.15368427994847297
INFO:flwr:		round 2: 0.14618950329720973
INFO:flwr:		round 3: 0.13766640634834768
INFO:flwr:		round 4: 0.14191509506106378
INFO:flwr:		round 5: 0.13836526349186898
INFO:flwr:	History (loss, centralized):
INFO:flwr:		round 0: 0.23037902109622954
INFO:flwr:		round 1: 0.20249070378541947
INFO:flwr:		round 2: 0.18032368122339248
INFO:flwr:		round 3: 0.1572258873164654
INFO:flwr:		round 4: 0.13654857886433602
INFO:flwr:		round 5: 0.1251989823669195
INFO:flwr:	History (metrics, distributed, evaluate):
INFO:flwr:	{'acc': [(1, 0.4495),
INFO:flwr:	         (2, 0.48949999999999994),
INFO:flwr:	         (3, 0.5157499999999999),
INFO:flwr:	         (4, 0.5115000000000001),
INFO:flwr:	         (5, 0.51025)]}
INFO:flwr:	History (metrics, centralized):
INFO:flwr:	{'accuracy': [(0, 0.1),
INFO:flwr:	              (1, 0.3473),
INFO:flwr:	              (2, 0.4098),
INFO:flwr:	              (3, 0.4599),
INFO:flwr:	              (4, 0.5172),
INFO:flwr:	              (5, 0.5618)]}
INFO:flwr:
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:ServerApp finished running.
DEBUG:flwr:Triggered stop event for Simulation Engine.
DEBUG:flwr:Terminated 16 actors
DEBUG:flwr:Terminated RayBackend
DEBUG:flwr:Stopping Simulation Engine now.
